{"name":"DEEPPRICE","statements":[{"name":"PROC DEEPPRICE","description":"The DEEPPRICE procedure estimates the average causal effect and performs policy evaluation and policy comparison by using deep neural networks (DNNs) when the treatment variable is continuous. The DEEPPRICE procedure applies DNNs via a two-step semiparametric framework and provides inferential results for the parameters of interest through the corresponding influence functions.The DEEPPRICE procedure can estimate two types of causal effects (or parameters of interest): the average intercept and the average slope.The procedure can also perform policy evaluation and policy comparison.PROC DEEPPRICE supports the SCORE statement, which enables you to save causal model specifications and estimation results for scoring and policy evaluation and comparison without needing to reestimate the DNNs for these models.","help":"PROC DEEPPRICE  options;\n\tDNN  options;\n\tID  variable;\n\tINFER  <options>;\n\tINSTRUMENT variable;\n\tMODEL  outcome-variable<=covariates></options>;\n\tSCORE  options;\n\tTMODEL  treatment-variable<=covariates></options>;","arguments":[{"name":"DATA=","description":"names the input data table for PROC DEEPPRICE to use.","help":"DATA=*libref.data-table*","type":"dataSet","arguments":[{"name":"libref","description":"refers to a collection of information that is defined in the LIBNAME statement and includes the library, which includes a path to the data, and a session identifier, which defaults to the active session but which can be explicitly defined in the LIBNAME statement.","type":"standalone"},{"name":"data-table","description":"specifies the name of the input data table.","type":"standalone"}]}]},{"name":"DNN","description":"The DNN statement specifies the parameters for the deep neural network (DNN). It includes options that are related to the DNNs for the treatment model and outcome model.","help":"DNN   &lt;options&gt;;","arguments":[{"name":"NODES=","optional":true,"description":"specifies the number of nodes for each hidden layer in the DNN, where each value in the numeric-list must be a positive number and values are separated by commas or spaces. The length of the list specifies the number of layers, and each value specifies the number of nodes for the corresponding layer. If the length of the list is denoted by n, then the ith (1<=i<=n) value denotes the number of nodes for the ith layer. By default, NODES=(32 32 32 32), which yields four hidden layers, with 32 nodes in each layer.","help":"NODES=(*numeric-list*)","type":"value"},{"name":"TRAIN=","optional":true,"description":"specifies the options that you can use to train the DNN.","help":"TRAIN=(*train-options*)","type":"value","arguments":[{"name":"GPU=","description":"specifies that the DNN perform calculations by using graphics processing unit (GPU) hardware.","help":"GPU=(*gpu-options*)","type":"choice","arguments":[{"name":"DEVICE=","description":"specifies a list of GPU devices to use.","help":"DEVICE=(*numeric-list*)","type":"value"},{"name":"DEVICES=","description":"specifies a list of GPU devices to use.","help":"DEVICES=(*numeric-list*)","type":"value"},{"name":"PRECISION=","description":"specifies the computational precision in forward-backward computations. By default, PRECISION=FP32.","help":"PRECISION=*FP16 | FP32*","type":"value","arguments":[{"name":"FP16","description":"uses half-precision (16-bit floating-point) computations to train and score the DNN. The lower computational precision in forward-backward computations can encourage tensor-core engagements.","type":"standalone"},{"name":"FP32","description":"uses single-precision (32-bit floating-point) computations to train and score the DNN.","type":"standalone"}]},{"name":"USEEXCLUSIVE","description":"specifies that only GPU devices be used.","type":"standalone"},{"name":"USETENSORRT","description":"enables the use of the TensorRT software development kit for fast inference.","type":"standalone"}]},{"name":"MISSING=","description":"specifies the policy to use for replacing missing input covariate values with imputed values.By default, MISSING=MEAN. The MISSING= option is for DNN training only. In the inference that is performed later, observations that have missing covariates are removed.","help":"MISSING=*MAX | MEAN | MIN | NONE*","type":"value","arguments":[{"name":"MAX","description":"replaces missing values with the maximum of the corresponding covariate.","type":"standalone"},{"name":"MEAN","description":"replaces missing values with the mean of the corresponding covariate.","type":"standalone"},{"name":"MIN","description":"replaces missing values with the minimum of the corresponding covariate.","type":"standalone"},{"name":"NONE","description":"ignores observations that have missing values.","type":"standalone"}]},{"name":"NTHREADS=","description":"specifies the number of threads to use for the DNN.","help":"NTHREADS=*number*","type":"value"},{"name":"OPTIMIZATION=","description":"specifies the settings for the optimization algorithm and optimization mode, as well as other settings, such as a seed, the maximum number of epochs, and so on.","help":"OPTIMIZATION=(*optimizer-options*)","type":"value","arguments":[{"name":"ALGORITHM=","description":"specifies the adaptive moment estimation (ADAM) optimization algorithm. For the adam-options, you can specify following options:","help":"ALGORITHM=ADAM(*adam-options*) | ADAMSGD(*adam-options*)","type":"choice","arguments":[{"name":"BETA1=","description":"specifies the exponential decay rate for the first moment in an ADAM learning algorithm. The number must be in the range [0, 1).By default, BETA1=0.9.","help":"BETA1=*number*","type":"value"},{"name":"BETA2=","description":"specifies the exponential decay rate for the second moment in an ADAM learning algorithm. The number must be in the range [0,1).By default, BETA2=0.999.","help":"BETA2=*number*","type":"value"},{"name":"CLIPGRADMAX=","description":"specifies the maximum gradient value. All gradient values that exceed the specified maximum value are set to the specified maximum value. The number should be greater than or equal to 0.","help":"CLIPGRADMAX=*number*","type":"value"},{"name":"CLIPGRADMIN=","description":"specifies the minimum gradient value. All gradient values that are less than the specified minimum value are set to the specified minimum value. The number should be less than or equal to 0.","help":"CLIPGRADMIN=*number*","type":"value"},{"name":"FCMPEARLYSTOPPING=","description":"specifies the function compiler (FCMP) early stopping function.","help":"FCMPEARLYSTOPPING=*symbol-name*","type":"value"},{"name":"FCMPES=","description":"specifies the function compiler (FCMP) early stopping function.","help":"FCMPES=*symbol-name*","type":"value"},{"name":"FCMPLEARNINGRATE=","description":"specifies the function compiler (FCMP) learning rate function.","help":"FCMPLEARNINGRATE=*symbol-name*","type":"value"},{"name":"FCMPLR=","description":"specifies the function compiler (FCMP) learning rate function.","help":"FCMPLR=*symbol-name*","type":"value"},{"name":"GAMMA=","description":"specifies the gamma value for the learning rate policy. The number must be between 0 and 100, exclusive. By default, GAMMA=0.1.","help":"GAMMA=*number*","type":"value"},{"name":"LEARNINGRATE=","description":"specifies the learning rate (a positive value) for stochastic gradient descent (SGD). By default, LEARNINGRATE=0.001.","help":"LEARNINGRATE=*number*","type":"value"},{"name":"LEARNINGRATEPOLICY=","description":"specifies the learning rate policy.By default, LEARNINGRATEPOLICY=FIXED.","type":"value","arguments":[{"name":"FIXED","description":"specifies a fixed learning rate.","type":"standalone"},{"name":"INV","description":"sets the learning rate option value after each epoch, according to the initial learning rate, the value of the GAMMA= option (γ ), and the value of the POWER= option. The rate is calculated as learningRate *(1 +γ* currentEpoch)^power","type":"standalone"},{"name":"MULTISTEP","description":"sets the learning rate after each of the epochs that you specify in the STEPS= option. The updated learning rate is the product of the previous learning rate value and the GAMMA= option value.","type":"standalone"},{"name":"POLY","description":"sets the learning rate option value after each epoch, according to the initial learning rate, the maximum number of epochs, and the value of the POWER= option. The rate is calculated as learningRate *(1 +γ* currentEpoch)^power","type":"standalone"},{"name":"STEP","description":"sets the learning rate option by multiplying the current learning rate option value by the GAMMA= option value. The number of steps is specified in the STEPSIZE= option. The configured learning rate is recalculated for each group of epochs, according to the step size","type":"standalone"}]},{"name":"LRPOLICY=","description":"specifies the learning rate policy.By default, LEARNINGRATEPOLICY=FIXED.","type":"value","arguments":[{"name":"FIXED","description":"specifies a fixed learning rate.","type":"standalone"},{"name":"INV","description":"sets the learning rate option value after each epoch, according to the initial learning rate, the value of the GAMMA= option (γ ), and the value of the POWER= option. The rate is calculated as learningRate *(1 +γ* currentEpoch)^power","type":"standalone"},{"name":"MULTISTEP","description":"sets the learning rate after each of the epochs that you specify in the STEPS= option. The updated learning rate is the product of the previous learning rate value and the GAMMA= option value.","type":"standalone"},{"name":"POLY","description":"sets the learning rate option value after each epoch, according to the initial learning rate, the maximum number of epochs, and the value of the POWER= option. The rate is calculated as learningRate *(1 +γ* currentEpoch)^power","type":"standalone"},{"name":"STEP","description":"sets the learning rate option by multiplying the current learning rate option value by the GAMMA= option value. The number of steps is specified in the STEPSIZE= option. The configured learning rate is recalculated for each group of epochs, according to the step size","type":"standalone"}]},{"name":"NOUSELOCKING","description":"computes gradients asynchronously with multiple threads.","type":"standalone"},{"name":"POWER=","description":"specifies the power for the learning rate policy. The number must be nonnegative. By default, POWER=0.75.","help":"POWER=*number*","type":"value"},{"name":"STEPS=","description":"specifies a list of epoch counts. When the current epoch matches one of the specified steps, the learning rate is multiplied by the value of the GAMMA= option. For example, if you specify STEPS=(5 9 13), then the learning rate is multiplied by the GAMMA= option value after the 5th, 9th, and 13th epochs.","help":"STEPS=(*numberic-list*)","type":"value"},{"name":"STEPSIZE=","description":"specifies the step size (a value greater than or equal to 1) when the learning rate policy is set to STEP. By default, STEPSIZE=10.","help":"STEPSIZE=*number*","type":"value"}]},{"name":"COMPRESSION=","description":"makes communication between workers more sparse for faster training by using gradient sparsification, a distributed training technique that makes stochastic gradients more sparse to reduce communication costs.","help":"COMPRESSION=(*compression-options*)","type":"value","arguments":[{"name":"LOCALMOMENTUM=","description":"etermines the strength of aggregation on the masked gradient elements when compression is used. The number must be in the range [0.1]. By default, LOCALMOMENTUM=0.95.","help":"LOCALMOMENTUM=*number*","type":"value"},{"name":"SAMPLERATIO=","description":"determines the number of samples for gradients in each layer. The number must be in the range [10^-5,0.2], By default, SAMPLERATIO=0.05.","help":"SAMPLERATIO=*number*","type":"value"},{"name":"SPARSITYRATIO=","description":"determines the number of samples for gradients in each layer. The number must be in the range [0, 1]. By default, SPARSITYRATIO=0.75.","help":"SPARSITYRATIO=*number*","type":"value"}]},{"name":"DROPOUT=","description":"specifies the probability that the output of a neuron in a fully connected layer will be set to 0 during training. This probability is recalculated each time an observation is processed. The number must be in the range [0; 1). By default, DROPOUT=0.","help":"DROPOUT=*number*","type":"value"},{"name":"DROPOUTINPUT=","description":"specifies the probability that an input variable will be set to 0 during training. This probability is recalculated each time an observation is processed. The number must be in the range[0; 1). By default, DROPOUTINPUT=0","help":"DROPOUTINPUT=*number*","type":"value"},{"name":"DROPOUTTYPE=","description":"specifies what type of dropout to use. By default, DROPOUTTYPE=STANDARD.","type":"value","arguments":[{"name":"INVERTED","description":"uses the inverted dropout, in which activations of some neurons are set to 0 and the remaining activations are scaled.","type":"standalone"},{"name":"STANDARD","description":"uses the standard dropout, in which activations of some neurons are set to 0 and the remaining activations are not scaled.","type":"standalone"}]},{"name":"FCONV=","description":"specifies the relative function convergence criterion. If the relative loss error, abs[(previous_loss-current_loss)/previous_loss], does not result in a change in the objective function, the optimization stops. The number must be greater than or equal to 0. By default, FCONV=0, and the relative function convergence is not checked.","help":"FCONV=*number*","type":"value"},{"name":"FREEZEBATCHNORMSTATS","description":"freezes statistics of all batch normalization layers.","help":"FREEZEBATCHNORMSTATS ","type":"standalone"},{"name":"FREEZEBNSTATS","description":"freezes statistics of all batch normalization layers.","type":"standalone"},{"name":"IGNORETRAININGERROR","description":"continues model training without interruption by ignoring the issue of training data observations containing invalid or missing variable data. If you omit this option, training stops and PROC DEEPPRICE terminates when bad input data are found.","type":"standalone"},{"name":"MAXEPOCHS=","description":"specifies the maximum number of epochs. The number must be greater than or equal to 1. By default, MAXEPOCHS=1. For SGD with a single-machine server, or for a session that uses one worker on a distributed server, one epoch is reached when the optimizer passes through the data one time. For a session that uses more than one worker, one epoch is reached when all the workers exchange the weights with the controller one time. The SYNCFREQ= option specifies the number of times that each worker passes through the data before exchanging weights with the controller. For the L-BFGS optimization algorithm with a full batch, each iteration might process more than one epoch, and the final number of epochs might exceed the parameter value for the maximum number of epochs.","help":"MAXEPOCHS=*number*","type":"value"},{"name":"MINIBATCHBUFSIZE=","description":"specifies the number of observations to buffer in memory (including input data and intermediate calculations) before processing the remaining records in the minibatch. The number must be greater than or equal to 1 and must be smaller than the number that you specify for the MINIBATCHSIZE= option. The MINIBATCHBUFSIZE= option setting is ignored when GPUs are used.","help":"MINIBATCHBUFSIZE=*number*","type":"value"},{"name":"MINIBATCHSIZE=","description":"specifies the number of observations per thread in a minibatch. The parameter controls the number of observations that are used per worker in each thread to compute the gradient, prior to updating the weights. Larger values use more memory. The number is greater than or equal to 1. By default, MINIBATCHSIZE=1. When you use synchronous SGD (the default), the total minibatch size is calculated as miniBatchSize * number of threads * number of workers. When you use asynchronous SGD by specifying the ELASTICSYNCFREQ= option, each worker trains its own local model. In this case, the total minibatch size for each worker is calculated as miniBatchSize * number of threads. You can specify values s for either the MINIBATCHSIZE= or TOTALMINIBATCHSIZE= option, but not for both.","help":"MINIBATCHSIZE=*number*","type":"value"},{"name":"MODE=","description":"specifies the optimization mode and the corresponding options.By default, MODE=SYNCHRONOUS.","help":"MODE=*optimization-mode*","type":"value","arguments":[{"name":"DOWNPOUR","description":"specifies the downpour optimization mode.","type":"standalone"},{"name":"ELASTIC","description":"specifies the elastic optimization mode","help":"ELASTIC&lt;(*elastic-options*)&gt;","type":"value","arguments":[{"name":"ALPHA=","description":"specifies the significance level (a value between 0 and 1, inclusive) that is used for elastic SGD. By default, ALPHA=0.","help":"ALPHA=*number*","type":"value"},{"name":"ELASTICALPHA=","description":"specifies the significance level (a value between 0 and 1, inclusive) that is used for elastic SGD. By default, ALPHA=0.","help":"ELASTICALPHA=*number*","type":"value"},{"name":"ELASTICSYNCFREQ=","description":"and controller for exchanging weights. You can exchange the weights more often than once each epoch by specifying a number that is less than the number of batches in an epoch. If the number is greater than the number of batches in an epoch, then the weights are exchanged once each epoch. By default, ELASTICSYNCFREQ=0.","help":"ELASTICSYNCFREQ=*number*","type":"value"},{"name":"SYNCFREQ=","description":"and controller for exchanging weights. You can exchange the weights more often than once each epoch by specifying a number that is less than the number of batches in an epoch. If the number is greater than the number of batches in an epoch, then the weights are exchanged once each epoch. By default, ELASTICSYNCFREQ=0.","help":"SYNCFREQ=*number*","type":"value"},{"name":"SYNCHRONOUS","description":"specifies the synchronous optimization mode. The SYNCFREQ= option, which is not required, specifies the synchronization frequency for SGD in terms of epochs. By default, SYNCFREQ=1","help":"SYNCHRONOUS&lt;(SYNCFREQ=*number*)&gt;","type":"standaloneOrValue"}]}]},{"name":"NOBNSRCLAYERWARNINGS","description":"suppresses the warning if the batch normalization source layer has an atypical type or activation setting.","help":"NOBNSRCLAYERWARNINGS ","type":"standalone"},{"name":"REGL1=","description":"specifies the weight for the L2 regularization term. The number must be greater than or equal to 0. By default, REGL2=0. At the default setting, PROC DEEPPRICE does not perform L2 regularization. Initial L2 weight values should be small (such as 1E–3). You can combine L2 regularization with L1 regularization.","help":"REGL1=*number*","type":"value"},{"name":"REGL2=","description":"specifies the weight for the L1 regularization term. The number must be greater than or equal to 0. By default, REGL1=0. At the default setting, PROC DEEPPRICE does not perform L1 regularization. Initial L1 weight values should be small (such as 1E–3). You can combine L1 regularization with L2 regularization.","help":"REGL2=*number*","type":"value"},{"name":"SEED=","description":"specifies the random number seed value for the random number generator in the L-BFGS optimization algorithm. Seed values less than or equal to 0 generate random number streams by using the time of day from the computer’s clock. Seed values greater than 0 generate reproducible random number sequences. By default, SEED=0.","help":"SEED=*number*","type":"value"},{"name":"SNAPSHOTFREQ=","description":"specifies the frequency for generating snapshots of the neural weights and storing the weights in a weight table during the training process. When you specify asynchronous SGD by specifying the ELASTICSYNCFREQ= option, PROC DEEPPRICE synchronizes all the weights before writing out the weights. The number must be greater than or equal to 0. By default, SNAPSHOTFREQ=0.","help":"SNAPSHOTFREQ=*number*","type":"value"},{"name":"STAGNATION=","description":"specifies the number of iterations to complete without improvement before stopping the optimization early. The number must be greater than or equal to 0. By default, STAGNATION=0; this turns off the stagnation monitoring.","help":"STAGNATION=*number*","type":"value"},{"name":"THRESHOLD=","description":"specifies the threshold that is used to determine whether the loss error or validation scores are improving or stagnating across iterations. The iteration does not improve when the magnitude of the score change between successive iterations is less than or equal to the absolute value of current_score * threshold value. When an iteration does not improve, the stagnation counter increments. Otherwise, the stagnation counter is set to 0. The number must be greater than or equal to 0. By default, THRESHOLD=10^-8","help":"THRESHOLD=*number*","type":"value"},{"name":"TOTALMINIBATCHSIZE=","description":"specifies the number of observations in a minibatch.You can use these options to control the number of observations that the DNN uses to compute the gradient prior to updating the weights. Larger values use more memory. If the specified size cannot be evenly divided by the number of threads (if you are using asynchronous SGD) or by the number of threads * number of workers (if you are using synchronous SGD), then PROC DEEPPRICE terminates with an error unless you specify the ROUND option. In that case, the total minibatch size is rounded up so that it is evenly divided. You can specify values for either the MINIBATCHSIZE= or TOTALMINIBATCHSIZE= option, but not for both.","help":"TOTALMINIBATCHSIZE=(*totalminibatchsize-options*)","type":"value","arguments":[{"name":"NONOTIFY","description":"suppresses the message that is printed if the total minibatch size is rounded up.","type":"standalone"},{"name":"ROUND","description":"rounds up the total minibatch size.","type":"standalone","arguments":[{"name":"SIZE=","description":"specifies the number of observations in a minibatch. The computation for the total minibatch size is nWorkers * miniBatchSize * nThreads. For example, a two-worker scenario with a minibatch size of 10 and 32 threads per worker has a total minibatch size of 2*32*10 = 640 records.","help":"SIZE=*number*","type":"value"},{"name":"N=","description":"specifies the number of observations in a minibatch. The computation for the total minibatch size is nWorkers * miniBatchSize * nThreads. For example, a two-worker scenario with a minibatch size of 10 and 32 threads per worker has a total minibatch size of 2*32*10 = 640 records.","help":"N=*number*","type":"value"}]}]}]},{"name":"OPTIMIZE=","description":"specifies the settings for the optimization algorithm and optimization mode, as well as other settings, such as a seed, the maximum number of epochs, and so on.","help":"OPTIMIZE=(*optimizer-options*)","type":"value","arguments":[{"name":"ALGORITHM=","description":"specifies the adaptive moment estimation (ADAM) optimization algorithm. For the adam-options, you can specify following options:","help":"ALGORITHM=ADAM(*adam-options*) | ADAMSGD(*adam-options*)","type":"choice","arguments":[{"name":"BETA1=","description":"specifies the exponential decay rate for the first moment in an ADAM learning algorithm. The number must be in the range [0, 1).By default, BETA1=0.9.","help":"BETA1=*number*","type":"value"},{"name":"BETA2=","description":"specifies the exponential decay rate for the second moment in an ADAM learning algorithm. The number must be in the range [0,1).By default, BETA2=0.999.","help":"BETA2=*number*","type":"value"},{"name":"CLIPGRADMAX=","description":"specifies the maximum gradient value. All gradient values that exceed the specified maximum value are set to the specified maximum value. The number should be greater than or equal to 0.","help":"CLIPGRADMAX=*number*","type":"value"},{"name":"CLIPGRADMIN=","description":"specifies the minimum gradient value. All gradient values that are less than the specified minimum value are set to the specified minimum value. The number should be less than or equal to 0.","help":"CLIPGRADMIN=*number*","type":"value"},{"name":"FCMPEARLYSTOPPING=","description":"specifies the function compiler (FCMP) early stopping function.","help":"FCMPEARLYSTOPPING=*symbol-name*","type":"value"},{"name":"FCMPES=","description":"specifies the function compiler (FCMP) early stopping function.","help":"FCMPES=*symbol-name*","type":"value"},{"name":"FCMPLEARNINGRATE=","description":"specifies the function compiler (FCMP) learning rate function.","help":"FCMPLEARNINGRATE=*symbol-name*","type":"value"},{"name":"FCMPLR=","description":"specifies the function compiler (FCMP) learning rate function.","help":"FCMPLR=*symbol-name*","type":"value"},{"name":"GAMMA=","description":"specifies the gamma value for the learning rate policy. The number must be between 0 and 100, exclusive. By default, GAMMA=0.1.","help":"GAMMA=*number*","type":"value"},{"name":"LEARNINGRATE=","description":"specifies the learning rate (a positive value) for stochastic gradient descent (SGD). By default, LEARNINGRATE=0.001.","help":"LEARNINGRATE=*number*","type":"value"},{"name":"LEARNINGRATEPOLICY=","description":"specifies the learning rate policy.By default, LEARNINGRATEPOLICY=FIXED.","type":"value","arguments":[{"name":"FIXED","description":"specifies a fixed learning rate.","type":"standalone"},{"name":"INV","description":"sets the learning rate option value after each epoch, according to the initial learning rate, the value of the GAMMA= option (γ ), and the value of the POWER= option. The rate is calculated as learningRate *(1 +γ* currentEpoch)^power","type":"standalone"},{"name":"MULTISTEP","description":"sets the learning rate after each of the epochs that you specify in the STEPS= option. The updated learning rate is the product of the previous learning rate value and the GAMMA= option value.","type":"standalone"},{"name":"POLY","description":"sets the learning rate option value after each epoch, according to the initial learning rate, the maximum number of epochs, and the value of the POWER= option. The rate is calculated as learningRate *(1 +γ* currentEpoch)^power","type":"standalone"},{"name":"STEP","description":"sets the learning rate option by multiplying the current learning rate option value by the GAMMA= option value. The number of steps is specified in the STEPSIZE= option. The configured learning rate is recalculated for each group of epochs, according to the step size","type":"standalone"}]},{"name":"LRPOLICY=","description":"specifies the learning rate policy.By default, LEARNINGRATEPOLICY=FIXED.","type":"value","arguments":[{"name":"FIXED","description":"specifies a fixed learning rate.","type":"standalone"},{"name":"INV","description":"sets the learning rate option value after each epoch, according to the initial learning rate, the value of the GAMMA= option (γ ), and the value of the POWER= option. The rate is calculated as learningRate *(1 +γ* currentEpoch)^power","type":"standalone"},{"name":"MULTISTEP","description":"sets the learning rate after each of the epochs that you specify in the STEPS= option. The updated learning rate is the product of the previous learning rate value and the GAMMA= option value.","type":"standalone"},{"name":"POLY","description":"sets the learning rate option value after each epoch, according to the initial learning rate, the maximum number of epochs, and the value of the POWER= option. The rate is calculated as learningRate *(1 +γ* currentEpoch)^power","type":"standalone"},{"name":"STEP","description":"sets the learning rate option by multiplying the current learning rate option value by the GAMMA= option value. The number of steps is specified in the STEPSIZE= option. The configured learning rate is recalculated for each group of epochs, according to the step size","type":"standalone"}]},{"name":"NOUSELOCKING","description":"computes gradients asynchronously with multiple threads.","type":"standalone"},{"name":"POWER=","description":"specifies the power for the learning rate policy. The number must be nonnegative. By default, POWER=0.75.","help":"POWER=*number*","type":"value"},{"name":"STEPS=","description":"specifies a list of epoch counts. When the current epoch matches one of the specified steps, the learning rate is multiplied by the value of the GAMMA= option. For example, if you specify STEPS=(5 9 13), then the learning rate is multiplied by the GAMMA= option value after the 5th, 9th, and 13th epochs.","help":"STEPS=(*numberic-list*)","type":"value"},{"name":"STEPSIZE=","description":"specifies the step size (a value greater than or equal to 1) when the learning rate policy is set to STEP. By default, STEPSIZE=10.","help":"STEPSIZE=*number*","type":"value"}]},{"name":"COMPRESSION=","description":"makes communication between workers more sparse for faster training by using gradient sparsification, a distributed training technique that makes stochastic gradients more sparse to reduce communication costs.","help":"COMPRESSION=(*compression-options*)","type":"value","arguments":[{"name":"LOCALMOMENTUM=","description":"etermines the strength of aggregation on the masked gradient elements when compression is used. The number must be in the range [0.1]. By default, LOCALMOMENTUM=0.95.","help":"LOCALMOMENTUM=*number*","type":"value"},{"name":"SAMPLERATIO=","description":"determines the number of samples for gradients in each layer. The number must be in the range [10^-5,0.2], By default, SAMPLERATIO=0.05.","help":"SAMPLERATIO=*number*","type":"value"},{"name":"SPARSITYRATIO=","description":"determines the number of samples for gradients in each layer. The number must be in the range [0, 1]. By default, SPARSITYRATIO=0.75.","help":"SPARSITYRATIO=*number*","type":"value"}]},{"name":"DROPOUT=","description":"specifies the probability that the output of a neuron in a fully connected layer will be set to 0 during training. This probability is recalculated each time an observation is processed. The number must be in the range [0; 1). By default, DROPOUT=0.","help":"DROPOUT=*number*","type":"value"},{"name":"DROPOUTINPUT=","description":"specifies the probability that an input variable will be set to 0 during training. This probability is recalculated each time an observation is processed. The number must be in the range[0; 1). By default, DROPOUTINPUT=0","help":"DROPOUTINPUT=*number*","type":"value"},{"name":"DROPOUTTYPE=","description":"specifies what type of dropout to use. By default, DROPOUTTYPE=STANDARD.","type":"value","arguments":[{"name":"INVERTED","description":"uses the inverted dropout, in which activations of some neurons are set to 0 and the remaining activations are scaled.","type":"standalone"},{"name":"STANDARD","description":"uses the standard dropout, in which activations of some neurons are set to 0 and the remaining activations are not scaled.","type":"standalone"}]},{"name":"FCONV=","description":"specifies the relative function convergence criterion. If the relative loss error, abs[(previous_loss-current_loss)/previous_loss], does not result in a change in the objective function, the optimization stops. The number must be greater than or equal to 0. By default, FCONV=0, and the relative function convergence is not checked.","help":"FCONV=*number*","type":"value"},{"name":"FREEZEBATCHNORMSTATS","description":"freezes statistics of all batch normalization layers.","help":"FREEZEBATCHNORMSTATS ","type":"standalone"},{"name":"FREEZEBNSTATS","description":"freezes statistics of all batch normalization layers.","type":"standalone"},{"name":"IGNORETRAININGERROR","description":"continues model training without interruption by ignoring the issue of training data observations containing invalid or missing variable data. If you omit this option, training stops and PROC DEEPPRICE terminates when bad input data are found.","type":"standalone"},{"name":"MAXEPOCHS=","description":"specifies the maximum number of epochs. The number must be greater than or equal to 1. By default, MAXEPOCHS=1. For SGD with a single-machine server, or for a session that uses one worker on a distributed server, one epoch is reached when the optimizer passes through the data one time. For a session that uses more than one worker, one epoch is reached when all the workers exchange the weights with the controller one time. The SYNCFREQ= option specifies the number of times that each worker passes through the data before exchanging weights with the controller. For the L-BFGS optimization algorithm with a full batch, each iteration might process more than one epoch, and the final number of epochs might exceed the parameter value for the maximum number of epochs.","help":"MAXEPOCHS=*number*","type":"value"},{"name":"MINIBATCHBUFSIZE=","description":"specifies the number of observations to buffer in memory (including input data and intermediate calculations) before processing the remaining records in the minibatch. The number must be greater than or equal to 1 and must be smaller than the number that you specify for the MINIBATCHSIZE= option. The MINIBATCHBUFSIZE= option setting is ignored when GPUs are used.","help":"MINIBATCHBUFSIZE=*number*","type":"value"},{"name":"MINIBATCHSIZE=","description":"specifies the number of observations per thread in a minibatch. The parameter controls the number of observations that are used per worker in each thread to compute the gradient, prior to updating the weights. Larger values use more memory. The number is greater than or equal to 1. By default, MINIBATCHSIZE=1. When you use synchronous SGD (the default), the total minibatch size is calculated as miniBatchSize * number of threads * number of workers. When you use asynchronous SGD by specifying the ELASTICSYNCFREQ= option, each worker trains its own local model. In this case, the total minibatch size for each worker is calculated as miniBatchSize * number of threads. You can specify values s for either the MINIBATCHSIZE= or TOTALMINIBATCHSIZE= option, but not for both.","help":"MINIBATCHSIZE=*number*","type":"value"},{"name":"MODE=","description":"specifies the optimization mode and the corresponding options.By default, MODE=SYNCHRONOUS.","help":"MODE=*optimization-mode*","type":"value","arguments":[{"name":"DOWNPOUR","description":"specifies the downpour optimization mode.","type":"standalone"},{"name":"ELASTIC","description":"specifies the elastic optimization mode","help":"ELASTIC&lt;(*elastic-options*)&gt;","type":"value","arguments":[{"name":"ALPHA=","description":"specifies the significance level (a value between 0 and 1, inclusive) that is used for elastic SGD. By default, ALPHA=0.","help":"ALPHA=*number*","type":"value"},{"name":"ELASTICALPHA=","description":"specifies the significance level (a value between 0 and 1, inclusive) that is used for elastic SGD. By default, ALPHA=0.","help":"ELASTICALPHA=*number*","type":"value"},{"name":"ELASTICSYNCFREQ=","description":"and controller for exchanging weights. You can exchange the weights more often than once each epoch by specifying a number that is less than the number of batches in an epoch. If the number is greater than the number of batches in an epoch, then the weights are exchanged once each epoch. By default, ELASTICSYNCFREQ=0.","help":"ELASTICSYNCFREQ=*number*","type":"value"},{"name":"SYNCFREQ=","description":"and controller for exchanging weights. You can exchange the weights more often than once each epoch by specifying a number that is less than the number of batches in an epoch. If the number is greater than the number of batches in an epoch, then the weights are exchanged once each epoch. By default, ELASTICSYNCFREQ=0.","help":"SYNCFREQ=*number*","type":"value"},{"name":"SYNCHRONOUS","description":"specifies the synchronous optimization mode. The SYNCFREQ= option, which is not required, specifies the synchronization frequency for SGD in terms of epochs. By default, SYNCFREQ=1","help":"SYNCHRONOUS&lt;(SYNCFREQ=*number*)&gt;","type":"standaloneOrValue"}]}]},{"name":"NOBNSRCLAYERWARNINGS","description":"suppresses the warning if the batch normalization source layer has an atypical type or activation setting.","help":"NOBNSRCLAYERWARNINGS ","type":"standalone"},{"name":"REGL1=","description":"specifies the weight for the L2 regularization term. The number must be greater than or equal to 0. By default, REGL2=0. At the default setting, PROC DEEPPRICE does not perform L2 regularization. Initial L2 weight values should be small (such as 1E–3). You can combine L2 regularization with L1 regularization.","help":"REGL1=*number*","type":"value"},{"name":"REGL2=","description":"specifies the weight for the L1 regularization term. The number must be greater than or equal to 0. By default, REGL1=0. At the default setting, PROC DEEPPRICE does not perform L1 regularization. Initial L1 weight values should be small (such as 1E–3). You can combine L1 regularization with L2 regularization.","help":"REGL2=*number*","type":"value"},{"name":"SEED=","description":"specifies the random number seed value for the random number generator in the L-BFGS optimization algorithm. Seed values less than or equal to 0 generate random number streams by using the time of day from the computer’s clock. Seed values greater than 0 generate reproducible random number sequences. By default, SEED=0.","help":"SEED=*number*","type":"value"},{"name":"SNAPSHOTFREQ=","description":"specifies the frequency for generating snapshots of the neural weights and storing the weights in a weight table during the training process. When you specify asynchronous SGD by specifying the ELASTICSYNCFREQ= option, PROC DEEPPRICE synchronizes all the weights before writing out the weights. The number must be greater than or equal to 0. By default, SNAPSHOTFREQ=0.","help":"SNAPSHOTFREQ=*number*","type":"value"},{"name":"STAGNATION=","description":"specifies the number of iterations to complete without improvement before stopping the optimization early. The number must be greater than or equal to 0. By default, STAGNATION=0; this turns off the stagnation monitoring.","help":"STAGNATION=*number*","type":"value"},{"name":"THRESHOLD=","description":"specifies the threshold that is used to determine whether the loss error or validation scores are improving or stagnating across iterations. The iteration does not improve when the magnitude of the score change between successive iterations is less than or equal to the absolute value of current_score * threshold value. When an iteration does not improve, the stagnation counter increments. Otherwise, the stagnation counter is set to 0. The number must be greater than or equal to 0. By default, THRESHOLD=10^-8","help":"THRESHOLD=*number*","type":"value"},{"name":"TOTALMINIBATCHSIZE=","description":"specifies the number of observations in a minibatch.You can use these options to control the number of observations that the DNN uses to compute the gradient prior to updating the weights. Larger values use more memory. If the specified size cannot be evenly divided by the number of threads (if you are using asynchronous SGD) or by the number of threads * number of workers (if you are using synchronous SGD), then PROC DEEPPRICE terminates with an error unless you specify the ROUND option. In that case, the total minibatch size is rounded up so that it is evenly divided. You can specify values for either the MINIBATCHSIZE= or TOTALMINIBATCHSIZE= option, but not for both.","help":"TOTALMINIBATCHSIZE=(*totalminibatchsize-options*)","type":"value","arguments":[{"name":"NONOTIFY","description":"suppresses the message that is printed if the total minibatch size is rounded up.","type":"standalone"},{"name":"ROUND","description":"rounds up the total minibatch size.","type":"standalone","arguments":[{"name":"SIZE=","description":"specifies the number of observations in a minibatch. The computation for the total minibatch size is nWorkers * miniBatchSize * nThreads. For example, a two-worker scenario with a minibatch size of 10 and 32 threads per worker has a total minibatch size of 2*32*10 = 640 records.","help":"SIZE=*number*","type":"value"},{"name":"N=","description":"specifies the number of observations in a minibatch. The computation for the total minibatch size is nWorkers * miniBatchSize * nThreads. For example, a two-worker scenario with a minibatch size of 10 and 32 threads per worker has a total minibatch size of 2*32*10 = 640 records.","help":"N=*number*","type":"value"}]}]}]},{"name":"OPTIMIZER=","description":"specifies the settings for the optimization algorithm and optimization mode, as well as other settings, such as a seed, the maximum number of epochs, and so on.","help":"OPTIMIZER=(*optimizer-options*)","type":"value","arguments":[{"name":"ALGORITHM=","description":"specifies the adaptive moment estimation (ADAM) optimization algorithm. For the adam-options, you can specify following options:","help":"ALGORITHM=ADAM(*adam-options*) | ADAMSGD(*adam-options*)","type":"choice","arguments":[{"name":"BETA1=","description":"specifies the exponential decay rate for the first moment in an ADAM learning algorithm. The number must be in the range [0, 1).By default, BETA1=0.9.","help":"BETA1=*number*","type":"value"},{"name":"BETA2=","description":"specifies the exponential decay rate for the second moment in an ADAM learning algorithm. The number must be in the range [0,1).By default, BETA2=0.999.","help":"BETA2=*number*","type":"value"},{"name":"CLIPGRADMAX=","description":"specifies the maximum gradient value. All gradient values that exceed the specified maximum value are set to the specified maximum value. The number should be greater than or equal to 0.","help":"CLIPGRADMAX=*number*","type":"value"},{"name":"CLIPGRADMIN=","description":"specifies the minimum gradient value. All gradient values that are less than the specified minimum value are set to the specified minimum value. The number should be less than or equal to 0.","help":"CLIPGRADMIN=*number*","type":"value"},{"name":"FCMPEARLYSTOPPING=","description":"specifies the function compiler (FCMP) early stopping function.","help":"FCMPEARLYSTOPPING=*symbol-name*","type":"value"},{"name":"FCMPES=","description":"specifies the function compiler (FCMP) early stopping function.","help":"FCMPES=*symbol-name*","type":"value"},{"name":"FCMPLEARNINGRATE=","description":"specifies the function compiler (FCMP) learning rate function.","help":"FCMPLEARNINGRATE=*symbol-name*","type":"value"},{"name":"FCMPLR=","description":"specifies the function compiler (FCMP) learning rate function.","help":"FCMPLR=*symbol-name*","type":"value"},{"name":"GAMMA=","description":"specifies the gamma value for the learning rate policy. The number must be between 0 and 100, exclusive. By default, GAMMA=0.1.","help":"GAMMA=*number*","type":"value"},{"name":"LEARNINGRATE=","description":"specifies the learning rate (a positive value) for stochastic gradient descent (SGD). By default, LEARNINGRATE=0.001.","help":"LEARNINGRATE=*number*","type":"value"},{"name":"LEARNINGRATEPOLICY=","description":"specifies the learning rate policy.By default, LEARNINGRATEPOLICY=FIXED.","type":"value","arguments":[{"name":"FIXED","description":"specifies a fixed learning rate.","type":"standalone"},{"name":"INV","description":"sets the learning rate option value after each epoch, according to the initial learning rate, the value of the GAMMA= option (γ ), and the value of the POWER= option. The rate is calculated as learningRate *(1 +γ* currentEpoch)^power","type":"standalone"},{"name":"MULTISTEP","description":"sets the learning rate after each of the epochs that you specify in the STEPS= option. The updated learning rate is the product of the previous learning rate value and the GAMMA= option value.","type":"standalone"},{"name":"POLY","description":"sets the learning rate option value after each epoch, according to the initial learning rate, the maximum number of epochs, and the value of the POWER= option. The rate is calculated as learningRate *(1 +γ* currentEpoch)^power","type":"standalone"},{"name":"STEP","description":"sets the learning rate option by multiplying the current learning rate option value by the GAMMA= option value. The number of steps is specified in the STEPSIZE= option. The configured learning rate is recalculated for each group of epochs, according to the step size","type":"standalone"}]},{"name":"LRPOLICY=","description":"specifies the learning rate policy.By default, LEARNINGRATEPOLICY=FIXED.","type":"value","arguments":[{"name":"FIXED","description":"specifies a fixed learning rate.","type":"standalone"},{"name":"INV","description":"sets the learning rate option value after each epoch, according to the initial learning rate, the value of the GAMMA= option (γ ), and the value of the POWER= option. The rate is calculated as learningRate *(1 +γ* currentEpoch)^power","type":"standalone"},{"name":"MULTISTEP","description":"sets the learning rate after each of the epochs that you specify in the STEPS= option. The updated learning rate is the product of the previous learning rate value and the GAMMA= option value.","type":"standalone"},{"name":"POLY","description":"sets the learning rate option value after each epoch, according to the initial learning rate, the maximum number of epochs, and the value of the POWER= option. The rate is calculated as learningRate *(1 +γ* currentEpoch)^power","type":"standalone"},{"name":"STEP","description":"sets the learning rate option by multiplying the current learning rate option value by the GAMMA= option value. The number of steps is specified in the STEPSIZE= option. The configured learning rate is recalculated for each group of epochs, according to the step size","type":"standalone"}]},{"name":"NOUSELOCKING","description":"computes gradients asynchronously with multiple threads.","type":"standalone"},{"name":"POWER=","description":"specifies the power for the learning rate policy. The number must be nonnegative. By default, POWER=0.75.","help":"POWER=*number*","type":"value"},{"name":"STEPS=","description":"specifies a list of epoch counts. When the current epoch matches one of the specified steps, the learning rate is multiplied by the value of the GAMMA= option. For example, if you specify STEPS=(5 9 13), then the learning rate is multiplied by the GAMMA= option value after the 5th, 9th, and 13th epochs.","help":"STEPS=(*numberic-list*)","type":"value"},{"name":"STEPSIZE=","description":"specifies the step size (a value greater than or equal to 1) when the learning rate policy is set to STEP. By default, STEPSIZE=10.","help":"STEPSIZE=*number*","type":"value"}]},{"name":"COMPRESSION=","description":"makes communication between workers more sparse for faster training by using gradient sparsification, a distributed training technique that makes stochastic gradients more sparse to reduce communication costs.","help":"COMPRESSION=(*compression-options*)","type":"value","arguments":[{"name":"LOCALMOMENTUM=","description":"etermines the strength of aggregation on the masked gradient elements when compression is used. The number must be in the range [0.1]. By default, LOCALMOMENTUM=0.95.","help":"LOCALMOMENTUM=*number*","type":"value"},{"name":"SAMPLERATIO=","description":"determines the number of samples for gradients in each layer. The number must be in the range [10^-5,0.2], By default, SAMPLERATIO=0.05.","help":"SAMPLERATIO=*number*","type":"value"},{"name":"SPARSITYRATIO=","description":"determines the number of samples for gradients in each layer. The number must be in the range [0, 1]. By default, SPARSITYRATIO=0.75.","help":"SPARSITYRATIO=*number*","type":"value"}]},{"name":"DROPOUT=","description":"specifies the probability that the output of a neuron in a fully connected layer will be set to 0 during training. This probability is recalculated each time an observation is processed. The number must be in the range [0; 1). By default, DROPOUT=0.","help":"DROPOUT=*number*","type":"value"},{"name":"DROPOUTINPUT=","description":"specifies the probability that an input variable will be set to 0 during training. This probability is recalculated each time an observation is processed. The number must be in the range[0; 1). By default, DROPOUTINPUT=0","help":"DROPOUTINPUT=*number*","type":"value"},{"name":"DROPOUTTYPE=","description":"specifies what type of dropout to use. By default, DROPOUTTYPE=STANDARD.","type":"value","arguments":[{"name":"INVERTED","description":"uses the inverted dropout, in which activations of some neurons are set to 0 and the remaining activations are scaled.","type":"standalone"},{"name":"STANDARD","description":"uses the standard dropout, in which activations of some neurons are set to 0 and the remaining activations are not scaled.","type":"standalone"}]},{"name":"FCONV=","description":"specifies the relative function convergence criterion. If the relative loss error, abs[(previous_loss-current_loss)/previous_loss], does not result in a change in the objective function, the optimization stops. The number must be greater than or equal to 0. By default, FCONV=0, and the relative function convergence is not checked.","help":"FCONV=*number*","type":"value"},{"name":"FREEZEBATCHNORMSTATS","description":"freezes statistics of all batch normalization layers.","help":"FREEZEBATCHNORMSTATS ","type":"standalone"},{"name":"FREEZEBNSTATS","description":"freezes statistics of all batch normalization layers.","type":"standalone"},{"name":"IGNORETRAININGERROR","description":"continues model training without interruption by ignoring the issue of training data observations containing invalid or missing variable data. If you omit this option, training stops and PROC DEEPPRICE terminates when bad input data are found.","type":"standalone"},{"name":"MAXEPOCHS=","description":"specifies the maximum number of epochs. The number must be greater than or equal to 1. By default, MAXEPOCHS=1. For SGD with a single-machine server, or for a session that uses one worker on a distributed server, one epoch is reached when the optimizer passes through the data one time. For a session that uses more than one worker, one epoch is reached when all the workers exchange the weights with the controller one time. The SYNCFREQ= option specifies the number of times that each worker passes through the data before exchanging weights with the controller. For the L-BFGS optimization algorithm with a full batch, each iteration might process more than one epoch, and the final number of epochs might exceed the parameter value for the maximum number of epochs.","help":"MAXEPOCHS=*number*","type":"value"},{"name":"MINIBATCHBUFSIZE=","description":"specifies the number of observations to buffer in memory (including input data and intermediate calculations) before processing the remaining records in the minibatch. The number must be greater than or equal to 1 and must be smaller than the number that you specify for the MINIBATCHSIZE= option. The MINIBATCHBUFSIZE= option setting is ignored when GPUs are used.","help":"MINIBATCHBUFSIZE=*number*","type":"value"},{"name":"MINIBATCHSIZE=","description":"specifies the number of observations per thread in a minibatch. The parameter controls the number of observations that are used per worker in each thread to compute the gradient, prior to updating the weights. Larger values use more memory. The number is greater than or equal to 1. By default, MINIBATCHSIZE=1. When you use synchronous SGD (the default), the total minibatch size is calculated as miniBatchSize * number of threads * number of workers. When you use asynchronous SGD by specifying the ELASTICSYNCFREQ= option, each worker trains its own local model. In this case, the total minibatch size for each worker is calculated as miniBatchSize * number of threads. You can specify values s for either the MINIBATCHSIZE= or TOTALMINIBATCHSIZE= option, but not for both.","help":"MINIBATCHSIZE=*number*","type":"value"},{"name":"MODE=","description":"specifies the optimization mode and the corresponding options.By default, MODE=SYNCHRONOUS.","help":"MODE=*optimization-mode*","type":"value","arguments":[{"name":"DOWNPOUR","description":"specifies the downpour optimization mode.","type":"standalone"},{"name":"ELASTIC","description":"specifies the elastic optimization mode","help":"ELASTIC&lt;(*elastic-options*)&gt;","type":"value","arguments":[{"name":"ALPHA=","description":"specifies the significance level (a value between 0 and 1, inclusive) that is used for elastic SGD. By default, ALPHA=0.","help":"ALPHA=*number*","type":"value"},{"name":"ELASTICALPHA=","description":"specifies the significance level (a value between 0 and 1, inclusive) that is used for elastic SGD. By default, ALPHA=0.","help":"ELASTICALPHA=*number*","type":"value"},{"name":"ELASTICSYNCFREQ=","description":"and controller for exchanging weights. You can exchange the weights more often than once each epoch by specifying a number that is less than the number of batches in an epoch. If the number is greater than the number of batches in an epoch, then the weights are exchanged once each epoch. By default, ELASTICSYNCFREQ=0.","help":"ELASTICSYNCFREQ=*number*","type":"value"},{"name":"SYNCFREQ=","description":"and controller for exchanging weights. You can exchange the weights more often than once each epoch by specifying a number that is less than the number of batches in an epoch. If the number is greater than the number of batches in an epoch, then the weights are exchanged once each epoch. By default, ELASTICSYNCFREQ=0.","help":"SYNCFREQ=*number*","type":"value"},{"name":"SYNCHRONOUS","description":"specifies the synchronous optimization mode. The SYNCFREQ= option, which is not required, specifies the synchronization frequency for SGD in terms of epochs. By default, SYNCFREQ=1","help":"SYNCHRONOUS&lt;(SYNCFREQ=*number*)&gt;","type":"standaloneOrValue"}]}]},{"name":"NOBNSRCLAYERWARNINGS","description":"suppresses the warning if the batch normalization source layer has an atypical type or activation setting.","help":"NOBNSRCLAYERWARNINGS ","type":"standalone"},{"name":"REGL1=","description":"specifies the weight for the L2 regularization term. The number must be greater than or equal to 0. By default, REGL2=0. At the default setting, PROC DEEPPRICE does not perform L2 regularization. Initial L2 weight values should be small (such as 1E–3). You can combine L2 regularization with L1 regularization.","help":"REGL1=*number*","type":"value"},{"name":"REGL2=","description":"specifies the weight for the L1 regularization term. The number must be greater than or equal to 0. By default, REGL1=0. At the default setting, PROC DEEPPRICE does not perform L1 regularization. Initial L1 weight values should be small (such as 1E–3). You can combine L1 regularization with L2 regularization.","help":"REGL2=*number*","type":"value"},{"name":"SEED=","description":"specifies the random number seed value for the random number generator in the L-BFGS optimization algorithm. Seed values less than or equal to 0 generate random number streams by using the time of day from the computer’s clock. Seed values greater than 0 generate reproducible random number sequences. By default, SEED=0.","help":"SEED=*number*","type":"value"},{"name":"SNAPSHOTFREQ=","description":"specifies the frequency for generating snapshots of the neural weights and storing the weights in a weight table during the training process. When you specify asynchronous SGD by specifying the ELASTICSYNCFREQ= option, PROC DEEPPRICE synchronizes all the weights before writing out the weights. The number must be greater than or equal to 0. By default, SNAPSHOTFREQ=0.","help":"SNAPSHOTFREQ=*number*","type":"value"},{"name":"STAGNATION=","description":"specifies the number of iterations to complete without improvement before stopping the optimization early. The number must be greater than or equal to 0. By default, STAGNATION=0; this turns off the stagnation monitoring.","help":"STAGNATION=*number*","type":"value"},{"name":"THRESHOLD=","description":"specifies the threshold that is used to determine whether the loss error or validation scores are improving or stagnating across iterations. The iteration does not improve when the magnitude of the score change between successive iterations is less than or equal to the absolute value of current_score * threshold value. When an iteration does not improve, the stagnation counter increments. Otherwise, the stagnation counter is set to 0. The number must be greater than or equal to 0. By default, THRESHOLD=10^-8","help":"THRESHOLD=*number*","type":"value"},{"name":"TOTALMINIBATCHSIZE=","description":"specifies the number of observations in a minibatch.You can use these options to control the number of observations that the DNN uses to compute the gradient prior to updating the weights. Larger values use more memory. If the specified size cannot be evenly divided by the number of threads (if you are using asynchronous SGD) or by the number of threads * number of workers (if you are using synchronous SGD), then PROC DEEPPRICE terminates with an error unless you specify the ROUND option. In that case, the total minibatch size is rounded up so that it is evenly divided. You can specify values for either the MINIBATCHSIZE= or TOTALMINIBATCHSIZE= option, but not for both.","help":"TOTALMINIBATCHSIZE=(*totalminibatchsize-options*)","type":"value","arguments":[{"name":"NONOTIFY","description":"suppresses the message that is printed if the total minibatch size is rounded up.","type":"standalone"},{"name":"ROUND","description":"rounds up the total minibatch size.","type":"standalone","arguments":[{"name":"SIZE=","description":"specifies the number of observations in a minibatch. The computation for the total minibatch size is nWorkers * miniBatchSize * nThreads. For example, a two-worker scenario with a minibatch size of 10 and 32 threads per worker has a total minibatch size of 2*32*10 = 640 records.","help":"SIZE=*number*","type":"value"},{"name":"N=","description":"specifies the number of observations in a minibatch. The computation for the total minibatch size is nWorkers * miniBatchSize * nThreads. For example, a two-worker scenario with a minibatch size of 10 and 32 threads per worker has a total minibatch size of 2*32*10 = 640 records.","help":"N=*number*","type":"value"}]}]}]},{"name":"RECORDSEED=","description":"specifies the random number seed for random record selection within a worker. Records are read in the order in which they are written in memory. Seed values that are less than or equal to 0 generate random number streams by using the time of day from the computer’s clock. Seed values greater than 0 generate reproducible random number sequences. By default, RECORDSEED=0; this disables random record selection.","help":"RECORDSEED=*number*","type":"value"},{"name":"SEED=","description":"specifies the random number seed value for the random number generator in SGD. Seed values that are less than or equal to 0 generate random number streams by using the time of day from the computer’s clock. Seed values greater than 0 generate reproducible random number sequences. By default, SEED=0.","help":"SEED=*number*","type":"value"},{"name":"TARGETMISSING=","description":"specifies the policy to use for replacing missing outcome variables with imputed values. By default, TARGETMISSING=MEAN. The TARGETMISSING= option is for DNN training only. In the inference that is performed later, observations that have missing outcome variables have missing values for residuals and for any estimates that need residuals.","help":"TARGETMISSING=*MAX | MEAN | MIN | NONE*","type":"value","arguments":[{"name":"MAX","description":"replaces missing values with the maximum.","type":"standalone"},{"name":"MEAN","description":"replaces missing values with the mean value.","type":"standalone"},{"name":"MIN","description":"replaces missing values with the minimum.","type":"standalone"},{"name":"NONE","description":"ignores observations that have missing target variables.","type":"standalone"}]}]}]},{"name":"ID","description":"The ID statement specifies the ID variable in the data set. The variable must have a unique value for each row in the input data table. If it does not, PROC DEEPPRICE issues an error.","help":"ID  *variable;*"},{"name":"INFER","description":"The INFER statement specifies the parameters for inference.","help":"INFER  options;","arguments":[{"name":"OUT=","optional":true,"description":"writes the estimation of the parameters of interest and the policy evaluation and policy comparison results to the specified output data table. libref.data-table is a two-level name, where libref refers to the library, and data-table specifies the name of the output data table.","help":"OUT=*libref.data-table*","type":"dataSet"},{"name":"OUTTEST=","optional":true,"description":"writes the estimation of the parameters of interest and the policy evaluation and policy comparison results to the specified output data table. libref.data-table is a two-level name, where libref refers to the library, and data-table specifies the name of the output data table.","help":"OUTTEST=*libref.data-table*","type":"dataSet"},{"name":"OUTDETAILS=","optional":true,"description":"writes the details of the estimation, includingα(⋅),β(⋅), p(⋅), the residual, and the influence functions for each unit, to the specified output data table. libref.data-table is a two-level name, where libref refers to the library, and data-table specifies the name of the output data table.","help":"OUTDETAILS= *libref.data-table*","type":"dataSet"},{"name":"POLICY=","optional":true,"description":"specifies the list of policy variables to be evaluated. All symbol names that the user specifies in the policy-variable-list must correspond to the policy variables; the symbol names are separated by spaces.","help":"POLICY=( *policy-variable-list*)","type":"value"},{"name":"POLICYCOMPARISON=","optional":true,"description":"specifies the options for policy comparison.","help":"POLICYCOMPARISON=(*comparison-options*)","type":"value","arguments":[{"name":"BASE=","description":"specifies a list of policy variables, each of which is used as a base policy in the policy comparison. All symbol names that the user specifies in the base-list must correspond to the policy variables; the symbol names are separated by spaces.","help":"BASE=(*base-list*)","type":"value"},{"name":"COMPARE=","description":"specifies a list of policy variables, each of which is compared to each base policy that you specify in the BASE= option. All symbol names that the user specifies in the compare-list must correspond to the policy variables; the symbol names are separated by spaces.","help":"COMPARE=(*compare-list*)","type":"value"}]}]},{"name":"INSTRUMENT","description":"The INSTRUMENT statement specifies the instrumental variable in the data table. The variable must be numeric in the input data table.The instrumental variable cannot be the same as any of the following: the ID variable that you specify in the ID statement; the treatment variables or covariates that you specify in the TREATMENT statement; or the outcome variable or covariates that you specify in the MODEL statement.When you are scoring by using the INT=, INETA0=, or INETA1= option in the SCORE statement for a treatment model that contains the instrumental variable, you must specify the INSTRUMENT statement in exactly same way (by using the same instrumental variable name) that you specify the INSTRUMENT statement used in the previous call of PROC DEEPPRICE that generates the corresponding OUTT=, OUTETA0=, or OUTETA1= table.","help":"INSTRUMENT *variable;*"},{"name":"MODEL","description":"The MODEL statement specifies options that are related to the outcome model and the DNN to fit the outcome model. The outcome-variable specifies the outcome variable of the outcome model. The covariates specify the covariates of the outcome model. If you omit the covariates, no DNN is trained for the outcome model.Covariate names must be separated by spaces. The treatment variable is automatically included in the outcome model as an independent variable.","help":"MODEL *outcome-variable &lt; =covariates&gt; &lt; /options &gt; ;*","arguments":[{"name":"DNN=","optional":true,"followsDelimiter":"/","description":"specifies the options of the DNN for the outcome model. If you omit the DNN= option and specify the DNN statement, the DNN for the outcome model has the options that you specify in the DNN statement. If you omit both the DNN= option and the DNN statement, the DNN for the outcome model uses the default option values. If you specify both the DNN= option and the DNN statement, the DNN statement is ignored for the outcome model. For the dnn-options, you can specify one or more options that are listed in the section “DNN Statement”. If no covariates are specified in the MODEL statement, the DNN= option is ignored.","help":"DNN=(*dnn-options*)","type":"value"}]},{"name":"SCORE","description":"The SCORE statement specifies options that are related to scoring data.","help":"SCORE  options;","arguments":[{"name":"INA=","optional":true,"description":"specifies the input data table that contains information about α(⋅) estimation. The data table must be an output data table that the OUTA= option produces in a previous call of PROC DEEPPRICE. When you specify the INA= option, you must also specify the INB= option, and the MODEL statement and the OUTA=, OUTB=, and OUTO= options in the SCORE statement are ignored. libref.data-table is a two-level name, where libref refers to the library, and data-table specifies the name of the input data table.","help":"INA=*libref.data-table*","type":"dataSet"},{"name":"INALPHA=","optional":true,"description":"specifies the input data table that contains information about α(⋅) estimation. The data table must be an output data table that the OUTA= option produces in a previous call of PROC DEEPPRICE. When you specify the INA= option, you must also specify the INB= option, and the MODEL statement and the OUTA=, OUTB=, and OUTO= options in the SCORE statement are ignored. libref.data-table is a two-level name, where libref refers to the library, and data-table specifies the name of the input data table.","help":"INALPHA=*libref.data-table*","type":"dataSet"},{"name":"INB=","optional":true,"description":"specifies the input data table that contains information about β(⋅) estimation. The data table must be an output data table that the OUTB= option produces in a previous call of PROC DEEPPRICE. When you specify the INB= option, you must also specify the INA= option, and the MODEL statement and the OUTA=, OUTB=, and OUTO= options in the SCORE statement are ignored.libref.data-table is a two-level name, where libref refers to the library, and data-table specifies the name of the input data table.","help":"INB=*libref.data-table*","type":"dataSet"},{"name":"INBETA=","optional":true,"description":"specifies the input data table that contains information about β(⋅) estimation. The data table must be an output data table that the OUTB= option produces in a previous call of PROC DEEPPRICE. When you specify the INB= option, you must also specify the INA= option, and the MODEL statement and the OUTA=, OUTB=, and OUTO= options in the SCORE statement are ignored.libref.data-table is a two-level name, where libref refers to the library, and data-table specifies the name of the input data table.","help":"INBETA=*libref.data-table*","type":"dataSet"},{"name":"INETA0=","optional":true,"description":"specifies the input data table that contains information about η₀(⋅) estimation. If you omit the INSTRUMENT statement, this option is ignored. The data table must be an output data table that the OUTETA0= option produces in a previous call of PROC DEEPPRICE. When you specify the INETA0= option, you must also specify the INETA1= option, and the TMODEL statement and the OUTETA0=, OUTETA1=, and OUTT= options in the SCORE statement are ignored.libref.data-table is a two-level name, where libref refers to the library, and data-table specifies the name of the input data table.","help":"INETA0=*libref.data-table*","type":"dataSet"},{"name":"INETA1=","optional":true,"description":"specifies the input data table that contains information about η₁(⋅) estimation. If you omit the INSTRUMENT statement, this option is ignored. The data table must be an output data table that the OUTETA1= option produces in a previous call of PROC DEEPPRICE. When you specify the INETA1= option, you must also specify the INETA1= option, and the TMODEL statement and the OUTETA0=, OUTETA1=, and OUTT= options in the SCORE statement are ignored.libref.data-table is a two-level name, where libref refers to the library, and data-table specifies the name of the input data table.","help":"INETA1=*libref.data-table*","type":"dataSet"},{"name":"INO=","optional":true,"description":"specifies the input data table that contains information about the outcome model. The data table must be an output data table that the OUTO= option produces in a previous call of PROC DEEPPRICE. When you specify the INO= option, the MODEL statement and the INA=, INB=, OUTA=, OUTB=, and OUTO= options in the SCORE statement are ignored.libref.data-table is a two-level name, where libref refers to the library, and data-table specifies the name of the input data table.","help":"INO=*libref.data-table*","type":"dataSet"},{"name":"INOUTCOME=","optional":true,"description":"specifies the input data table that contains information about the outcome model. The data table must be an output data table that the OUTO= option produces in a previous call of PROC DEEPPRICE. When you specify the INO= option, the MODEL statement and the INA=, INB=, OUTA=, OUTB=, and OUTO= options in the SCORE statement are ignored.libref.data-table is a two-level name, where libref refers to the library, and data-table specifies the name of the input data table.","help":"INOUTCOME=*libref.data-table*","type":"dataSet"},{"name":"INT=","optional":true,"description":"specifies the input data table that contains information about the treatment model. The data table must be an output data table that the OUTT= option produces in a previous call of PROC DEEPPRICE. When you specify the INT= option, the TMODEL statement and the OUTT= option in the SCORE statement are ignored.libref.data-table is a two-level name, where libref refers to the library, and data-table specifies the name of the input data table.","help":"INT=*libref.data-table*","type":"dataSet"},{"name":"INPROPENSITYSCORE=","optional":true,"description":"specifies the input data table that contains information about the treatment model. The data table must be an output data table that the OUTT= option produces in a previous call of PROC DEEPPRICE. When you specify the INT= option, the TMODEL statement and the OUTT= option in the SCORE statement are ignored.libref.data-table is a two-level name, where libref refers to the library, and data-table specifies the name of the input data table.","help":"INPROPENSITYSCORE=*libref.data-table*","type":"dataSet"},{"name":"OUTA=","optional":true,"description":"writes the α(⋅) estimation information to the specified output data table. The output data table consists of binary large object (BLOB) columns to store the binary data. Do not edit the content of this output data table. If you run the PRINT procedure on this data table, you get a row of zeros or meaningless characters, because the data type of each column is binary. libref.data-table is a two-level name, where libref refers to the library, and data-table specifies the name of the output data table.","help":"OUTA=*libref.data-table*","type":"dataSet"},{"name":"OUTALPHA=","optional":true,"description":"writes the α(⋅) estimation information to the specified output data table. The output data table consists of binary large object (BLOB) columns to store the binary data. Do not edit the content of this output data table. If you run the PRINT procedure on this data table, you get a row of zeros or meaningless characters, because the data type of each column is binary. libref.data-table is a two-level name, where libref refers to the library, and data-table specifies the name of the output data table.","help":"OUTALPHA=*libref.data-table*","type":"dataSet"},{"name":"OUTB=","optional":true,"description":"writes the β(⋅) estimation information to the specified output data table. The output data table consists of binary large object (BLOB) columns to store the binary data. Do not edit the content of this output data table. If you run the PRINT procedure on this data table, you get a row of zeros or meaningless characters, because the data type of each column is binary.libref.data-table is a two-level name, where libref refers to the library, and data-table specifies the name of the output data table.","help":"OUTB=*libref.data-table*","type":"dataSet"},{"name":"OUTBETA=","optional":true,"description":"writes the β(⋅) estimation information to the specified output data table. The output data table consists of binary large object (BLOB) columns to store the binary data. Do not edit the content of this output data table. If you run the PRINT procedure on this data table, you get a row of zeros or meaningless characters, because the data type of each column is binary.libref.data-table is a two-level name, where libref refers to the library, and data-table specifies the name of the output data table.","help":"OUTBETA=*libref.data-table*","type":"dataSet"},{"name":"OUTETA0=","optional":true,"description":"writes the η₀(⋅) estimation information to the specified output data table. The output data table consists of binary large object (BLOB) columns to store the binary data. Do not edit the content of this output data table. If you run the PRINT procedure on this data table, you get a row of zeros or meaningless characters, because the data type of each column is binary.libref.data-table is a two-level name, where libref refers to the library, and data-table specifies the name of the output data table.","help":"OUTETA0=*libref.data-table*","type":"dataSet"},{"name":"OUTETA1=","optional":true,"description":"writes the η₁(⋅) estimation information to the specified output data table. The output data table consists of binary large object (BLOB) columns to store the binary data. Do not edit the content of this output data table. If you run the PRINT procedure on this data table, you get a row of zeros or meaningless characters, because the data type of each column is binary.libref.data-table is a two-level name, where libref refers to the library, and data-table specifies the name of the output data table.","help":"OUTETA1=*libref.data-table*","type":"dataSet"},{"name":"OUTO=","optional":true,"description":"writes the outcome model information to the specified output data table. The output data table consists of binary large object (BLOB) columns to store the binary data. Do not edit the content of this output data table. If you run the PRINT procedure on this data table, you get a row of zeros or meaningless characters, because the data type of each column is binary.libref.data-table is a two-level name, where libref refers to the library, and data-table specifies the name of the output data table.","help":"OUTO=*libref.data-table*","type":"dataSet"},{"name":"OUTOUTCOME=","optional":true,"description":"writes the outcome model information to the specified output data table. The output data table consists of binary large object (BLOB) columns to store the binary data. Do not edit the content of this output data table. If you run the PRINT procedure on this data table, you get a row of zeros or meaningless characters, because the data type of each column is binary.libref.data-table is a two-level name, where libref refers to the library, and data-table specifies the name of the output data table.","help":"OUTOUTCOME=*libref.data-table*","type":"dataSet"},{"name":"OUTT=","optional":true,"description":"writes the treatment model information to the specified output data table. The output data table consists of binary large object (BLOB) columns to store the binary data. Do not edit the content of this output data table. If you run the PRINT procedure on this data table, you get a row of zeros or meaningless characters, because the data type of each column is binary.libref.data-table is a two-level name, where libref refers to the library, and data-table specifies the name of the output data table.","help":"OUTT=*libref.data-table*","type":"dataSet"},{"name":"OUTPROPENSITYSCORE=","optional":true,"description":"writes the treatment model information to the specified output data table. The output data table consists of binary large object (BLOB) columns to store the binary data. Do not edit the content of this output data table. If you run the PRINT procedure on this data table, you get a row of zeros or meaningless characters, because the data type of each column is binary.libref.data-table is a two-level name, where libref refers to the library, and data-table specifies the name of the output data table.","help":"OUTPROPENSITYSCORE=*libref.data-table*","type":"dataSet"}]},{"name":"TMODEL","description":"The TMODEL statement specifies options that are related to the treatment model and the DNN that is used to fit the treatment model. The treatment-variable specifies the treatment variable of the treatment model. It corresponds to one numeric variable in the input data table. The covariates specify the covariates of the treatment model. All covariate names are separated by spaces. If you omit the covariates and also omit the INSTRUMENT statement, no DNN is trained for the treatment model.","help":"TMODEL *treatment-variable &lt; =covariates &gt; &lt; /options &gt;  ;*","arguments":[{"name":"DNN=","optional":true,"followsDelimiter":"/","description":"specifies the options of the DNN that is used to fit the treatment model. If you omit the DNN= option and specify the DNN statement, the DNN for the treatment model has the options that you specify in the DNN statement. If you omit both the DNN= option and the DNN statement, the DNN for the treatment model uses the default option values. If you specify both the DNN= option and the DNN statement, the DNN statement is ignored for the treatment model.For the dnn-options, you can specify one or more options that are listed in the section “DNN Statement\".If no covariates are specified in the TMODEL statement, the DNN= option is ignored.","help":"DNN=(*dnn-options*)","type":"value"}]}],"supportSiteInformation":{"docsetId":"casecon","docsetVersion":"latest","docsetTargetFile":"casecon_deepprice_toc.htm"}}