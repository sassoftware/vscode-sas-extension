{"name":"GPREG","statements":[{"name":"PROC GPREG","description":"The GPREG procedure performs Gaussian process regression (GPR) for very large data sets in SAS Viya. GPR assumes that the predictions are based on a latent function that is a posterior distribution of a Gaussian process prior and a Gaussian noise likelihood. This latent function is adaptively learned on the basis of the observed data input in the reproducing kernel Hilbert space (RKHS), which is defined by the kernel of the Gaussian process. Thus it provides good regression performance. This procedure uses stochastic variational inference for Gaussian process models, which can process large data sets fast (Hensman, Fusi, and Lawrence 2013), and where the Gaussian processes can be variationally decomposed to depend on a set of globally relevant inducing variables that factorize the model in the manner necessary to perform variational inference in parallel. This enables you to apply Gaussian process models to data sets that contain millions of data points.","help":"PROC GPREG <options>;                 \n\tAUTOTUNE <options>;                 \n\tDISPLAY <table-list></ options>;                 \n\tDISPLAYOUT table-spec-list </ options>;                 \n\tINPUT variables<LEVEL= INTERVAL>;                 \n\tKERNEL kernel-type <options>;                 \n\tOPTIMIZATION <optimization-algorithm><options>;                 \n\tOUTPUT  OUT=libref.data-table<options>;                 \n\tPARTITION <partition-options>;                 \n\tSAVESTATE  RSTORE=libref.data-table;                 \n\tTARGET variables<LEVEL= INTERVAL>;                 ","arguments":[{"name":"APPLYROWORDER","optional":true,"description":"uses a data distribution and row order as determined by a previous partition action call.","type":"standalone"},{"name":"DATA=","optional":true,"description":"names the input data table for PROC GPREG to use. The default is the most recently created data table. libref.data-table is a two-level name, where libref refers to a collection of information that is defined in the LIBNAME statement and includes the library, which includes a path to the data, and a session identifier, which defaults to the active session but which can be explicitly defined in the LIBNAME statement. data-table specifies the name of the input data table.","help":"DATA=*libref.data-table*","type":"dataSet"},{"name":"FIXINDUCINGPOINTS","optional":true,"description":"specifies that the inducing points in the sparse Gaussian process are to be fixed or adjusted during the optimization. If you omit this option, PROC GPREG adjusts the locations of the inducing points during the iterations of the model inference.","type":"standalone"},{"name":"NINDUCINGPOINTS=","optional":true,"description":"specifies the number of inducing points to use in the sparse Gaussian process. The number must be a positive integer. When you specify a larger number, the PROC GPREG provides a more precise prediction results for the regression, but at a slower processing speed. When you specify a smaller number, the result are less accurate but the processing is faster. By default, NINDUCINGPOINTS=100.","help":"NINDUCINGPOINTS=*number*","type":"value"},{"name":"NTHREADS=","optional":true,"description":"specifies the number of threads to use in the computation. The default value is the number of CPUs available in the machine.","help":"NTHREADS=*number-of-threads*","type":"value"},{"name":"OUTINDUCINGPOINTS=","optional":true,"description":"creates the data table that contains the inducing points to use in the sparse Gaussian process. If you also specify the FIXINDUCINGPOINTS option, this table contains the cluster centroids of the k-means used as the initial inducing points. Otherwise, the table contains the locations of the inducing points of the last iteration of the model inference. libref.data-table is a two-level name, where libref refers to the library, and data-table specifies the name of the output data table.","help":"OUTINDUCINGPOINTS=*libref.data-table*","type":"dataSet"},{"name":"OUTVARIATIONALCOV=","optional":true,"description":"creates the data table that contains the covariance to use in the sparse Gaussian process. The table contains the covariance of the Gaussian process used in the last iteration of the model inference. libref.data-table is a two-level name, where libref refers to the library, and data-table specifies the name of the output data table.","help":"OUTVARIATIONALCOV=*libref.data-table*","type":"dataSet"},{"name":"SEED=","optional":true,"description":"specifies an integer to be used to start the pseudorandom number generator. If you do not specify a seed or if you specify a value less than or equal to 0, the seed is generated by reading the time of day from the computer’s clock.","help":"SEED=*number*","type":"value"}]},{"name":"AUTOTUNE","description":"searches for the best combination of some option values in the PROC GPREG, KERNEL, and OPTIMIZATION statement.","help":"AUTOTUNE &lt;options&gt;;                                              ","arguments":[{"name":"TUNINGPARAMETERS=","optional":true,"aliases":["TUNEPARMS="],"description":"specifies which parameters to tune and which ranges to tune over. If USEPARAMETERS=STANDARD, this option is ignored.","help":"TUNINGPARAMETERS=(*suboption* |...| &lt;*suboption*&gt;)","type":"value","arguments":[{"name":"ALGORITHM","description":"specifies the optimization algorithm.","help":"ALGORITHM (VALUES=*value-list* INIT=*value* EXCLUDE)","type":"value","arguments":[{"name":"VALUES=","description":"specifies a list of algorithm types to use in the tuning process, where value-list is a space-separated list of ADAM and SGD","help":"VALUES=*value-list*","type":"value"},{"name":"INIT=","description":"specifies the initial algorithm type in the tuning process. By default, INIT=SGD.","help":"INIT=ADAM|SGD","type":"choice"},{"name":"EXCLUDE","description":"excludes the algorithm type from the tuning process. If you specify this suboption, the VALUES= and INIT= suboptions are ignored.","type":"standalone"}]},{"name":"ARD","description":"uses automatic relevance determination in the kernel function.","help":"ARD (VALUES=*value-list* INIT=*value* EXCLUDE)","type":"value","arguments":[{"name":"VALUES=","description":"specifies a list of values to use in the tuning process, where value-list is a space-separated list of FALSE and TRUE.","help":"VALUES=*value-list*","type":"value"},{"name":"INIT=","description":"specifies the initial automatic relevance determination value in the tuning process. By default, INIT=FALSE.","help":"INIT=FALSE|TRUE","type":"choice"},{"name":"EXCLUDE","description":"excludes the automatic relevance determination from the tuning process. If you specify this suboption, the VALUES= and INIT= suboptions are ignored.","type":"standalone"}]},{"name":"JITTERMAXITERS","description":"specifies the maximum number of iterations for jitter Cholesky decomposition.","help":"JITTERMAXITERS (LB=*number* UB=*number* VALUES=*value-list* INIT=*number* EXCLUDE)","type":"value","arguments":[{"name":"LB=","description":"specifies the minimum jitter iterations to use in the tuning process, where number is a nonnegative integer. If you specify this suboption, you cannot specify the VALUES= suboption. By default, LB=5.","help":"LB=*number*","type":"value"},{"name":"UB=","description":"specifies the maximum jitter iterations to use in the tuning process, where number is a nonnegative integer. If you specify this suboption, you cannot specify the VALUES= suboption. By default, UB=15.","help":"UB=*number*","type":"value"},{"name":"VALUES=","description":"specifies a list of jitter iterations to use in the tuning process, where value-list is a space-separated list of nonnegative integers. If you specify this suboption, you cannot specify either the LB= or UB= suboption.","help":"VALUES=*value-list*","type":"value"},{"name":"INIT=","description":"specifies the initial jitter iterations in the tuning process. By default, INIT=10.","help":"INIT=*number*","type":"value"},{"name":"EXCLUDE","description":"excludes the jitter iterations from the tuning process. If you specify this suboption, any specified LB=, UB=, VALUES=, and INIT= suboptions are ignored.","type":"standalone"}]},{"name":"KERNEL","description":"specifies the kernel type to be used in the Gaussian process.","help":"KERNEL (VALUES=*value-list* INIT=*value* EXCLUDE)","type":"value","arguments":[{"name":"VALUES=","description":"specifies a list of kernel types to use in the tuning process, where value-list is a space-separated list of LINEAR, MATERN32, MATERN52, PERIODIC, and RBF.","help":"VALUES=*value-list*","type":"value"},{"name":"INIT=","description":"specifies the initial kernel type in the tuning process. By default, INIT=RBF.","help":"INIT={LINEAR|MATERN32|MATERN52|PERIODIC|RBF}","type":"choice"},{"name":"EXCLUDE","description":"excludes the kernel from the tuning process. If you specify this suboption, the VALUES= and INIT= suboptions are ignored.","type":"standalone"}]},{"name":"LEARNINGRATE","description":"specifies the learning rate parameter that the optimizaton algorithm uses.","help":"LEARNINGRATE (LB=*number* UB=*number* VALUES=*value-list* INIT=*number* EXCLUDE)","type":"value","arguments":[{"name":"LB=","description":"specifies the minimum learning rate to use in the tuning process, where number is a nonnegative double. If you specify this suboption, you cannot specify the VALUES= suboption. By default, LB=1E-7.","help":"LB=*number*","type":"value"},{"name":"UB=","description":"specifies the maximum learning rate to use in the tuning process, where number is a nonnegative double. If you specify this suboption, you cannot specify the VALUES= suboption. By default, UB=2.","help":"UB=*number*","type":"value"},{"name":"VALUES=","description":"specifies a list of learning rate to use in the tuning process, where value-list is a space-separated list of nonnegative doubles. If you specify this suboption, you cannot specify either the LB= or UB= suboption.","help":"VALUES=*value-list*","type":"value"},{"name":"INIT=","description":"specifies the initial learning rate in the tuning process. By default, INIT=1E-3.","help":"INIT=*number*","type":"value"},{"name":"EXCLUDE","description":"excludes the learning rate from the tuning process. If you specify this suboption, any specified LB=, UB=, VALUES=, and INIT= suboptions are ignored.","type":"standalone"}]},{"name":"MINIBATCHSIZE","description":"specifies the value for size of the minibatch in the gradient descent.","help":"MINIBATCHSIZE (LB=*number* UB=*number* VALUES=*value-list* INIT=*number* EXCLUDE)","type":"value","arguments":[{"name":"LB=","description":"specifies the minimum minibatch size to use in the tuning process, where number is a nonnegative integer. If you specify this suboption, you cannot specify the VALUES= suboption. By default, LB=1.","help":"LB=*number*","type":"value"},{"name":"UB=","description":"specifies the maximum minibatch size to use in the tuning process, where number is a nonnegative integer. If you specify this suboption, you cannot specify the VALUES= suboption. By default, UB=64.","help":"UB=*number*","type":"value"},{"name":"VALUES=","description":"specifies a list of minibatch sizes to use in the tuning process, where value-list is a space-separated list of nonnegative integers. If you specify this suboption, you cannot specify either the LB= or UB= suboption.","help":"VALUES=*value-list*","type":"value"},{"name":"INIT=","description":"specifies the initial minibatch size in the tuning process. By default, INIT=1.","help":"INIT=*number*","type":"value"},{"name":"EXCLUDE","description":"excludes the minibatch size from the tuning process. If you specify this suboption, any specified LB=, UB=, VALUES=, and INIT= suboptions are ignored.","type":"standalone"}]},{"name":"MOMENTUM","description":"specifies the value for momentum in the optimization.","help":"MOMENTUM (LB=*number* UB=*number* VALUES=*value-list* INIT=*number* EXCLUDE)","type":"value","arguments":[{"name":"LB=","description":"specifies the minimum momentum to use in the tuning process, where number is a nonnegative double. If you specify this suboption, you cannot specify the VALUES= suboption. By default, LB=0.","help":"LB=*number*","type":"value"},{"name":"UB=","description":"specifies the maximum momentum to use in the tuning process, where number is a nonnegative double. If you specify this suboption, you cannot specify the VALUES= suboption. By default, UB=0.99.","help":"UB=*number*","type":"value"},{"name":"VALUES=","description":"specifies a list of momentum to use in the tuning process, where value-list is a space-separated list of nonnegative doubles. If you specify this suboption, you cannot specify either the LB= or UB= suboption.","help":"VALUES=*value-list*","type":"value"},{"name":"INIT=","description":"specifies the momentum rate in the tuning process. By default, INIT=0.1.","help":"INIT=*number*","type":"value"},{"name":"EXCLUDE","description":"excludes the momentum from the tuning process. If you specify this suboption, any specified LB=, UB=, VALUES=, and INIT= suboptions are ignored.","type":"standalone"}]},{"name":"NINDUCINGPOINTS","description":"specifies the number of inducing points in the Gaussian process","help":"NINDUCINGPOINTS (LB=*number* UB=*number* VALUES=*value-list* INIT=*number* EXCLUDE)","type":"value","arguments":[{"name":"LB=","description":"specifies the minimum number of inducing points to use in the tuning process, where number is a nonnegative integer. If you specify this suboption, you cannot specify the VALUES= suboption. By default, LB=25.","help":"LB=*number*","type":"value"},{"name":"UB=","description":"specifies the maximum number of inducing points to use in the tuning process, where number is a nonnegative integer. If you specify this suboption, you cannot specify the VALUES= suboption. By default, UB=150.","help":"UB=*number*","type":"value"},{"name":"VALUES=","description":"specifies a list of number of inducing points to use in the tuning process, where value-list is a space-separated list of nonnegative integers. If you specify this suboption, you cannot specify either the LB= or UB= suboption.","help":"VALUES=*value-list*","type":"value"},{"name":"INIT=","description":"specifies the initial number of inducing points in the tuning process. By default, INIT=100.","help":"INIT=*number*","type":"value"},{"name":"EXCLUDE","description":"excludes the number of inducing points from the tuning process. If you specify this suboption, any specified LB=, UB=, VALUES=, and INIT= suboptions are ignored.","type":"standalone"}]},{"name":"TUNEADAPTIVEDECAY","description":"specifies the rate at which the second moment of the gradient is decayed.","help":"TUNEADAPTIVEDECAY (LB=*number* UB=*number* VALUES=*value-list* INIT=*number* EXCLUDE)","type":"value","arguments":[{"name":"LB=","description":"specifies the minimum adaptive decay to use in the tuning process, where number is a nonnegative double. If you specify this suboption, you cannot specify the VALUES= suboption. By default, LB=0.","help":"LB=*number*","type":"value"},{"name":"UB=","description":"specifies the maximum adaptive decay to use in the tuning process, where number is a nonnegative double. If you specify this suboption, you cannot specify the VALUES= suboption. By default, UB=0.99.","help":"UB=*number*","type":"value"},{"name":"VALUES=","description":"specifies a list of adaptive decays to use in the tuning process, where value-list is a space-separated list of nonnegative doubles. If you specify this suboption, you cannot specify either the LB= or UB= suboption.","help":"VALUES=*value-list*","type":"value"},{"name":"INIT=","description":"specifies the initial adaptive decay in the tuning process. By default, INIT=0.95.","help":"INIT=*number*","type":"value"},{"name":"EXCLUDE","description":"excludes the adaptive decay from the tuning process. If you specify this suboption, any specified LB=, UB=, VALUES=, and INIT= suboptions are ignored.","type":"standalone"}]},{"name":"TUNEADAPTIVERATE","description":"uses an adaptive learning rate in the gradient descent.","help":"TUNEADAPTIVERATE (VALUES=*value-list* INIT=*value* EXCLUDE)","type":"value","arguments":[{"name":"VALUES=","description":"specifies a list of values to use in the tuning process, where value-list is a space-separated list of the values FALSE and TRUE.","help":"VALUES=*value-list*","type":"value"},{"name":"INIT=","description":"specifies the initial adaptive rate value in the tuning process. By default, INIT=FALSE.","help":"INIT={FALSE|TRUE}","type":"choice"},{"name":"EXCLUDE","description":"excludes the adaptive rate from the tuning process. If you specify this suboption, the VALUES= and INIT= suboptions are ignored.","type":"standalone"}]}]}]},{"name":"DISPLAY","description":"enables you to specify a list of display tables to display or exclude. This statement is similar to the ODS SELECT, ODS EXCLUDE, and ODS TRACE statements. However, the DISPLAY statement can improve performance when a large number of tables could be generated (such as in BY-group processing). The procedure processes the DISPLAY statement on a CAS server and thus sends only a subset of ODS tables to the SAS client. Because ODS statements are processed on a SAS client, first all the generated display tables are sent to the client, and then the client creates a subset. If you use both DISPLAY and ODS statements together, the DISPLAY statement takes precedence over the ODS statements. Note that the ODS EXCLUDE statement processes tables that are sent to the client after they have been filtered by the DISPLAY statement. In some cases, it might appear that the ODS EXCLUDE statement is taking precedence because it can further filter the tables. You can specify the table-list as a list of table names, paths, partial pathnames, and regular expressions.","help":"DISPLAY &lt;table-list&gt;&lt;/ options&gt;;                                              ","arguments":[{"name":"CASESENSITIVE","optional":true,"followsDelimiter":"/","description":"performs a case-sensitive comparison of table names in the table-list to display table names when tables are subsetted for display. To preserve case, you must enclose table names in the table-list in quotation marks.","type":"standalone"},{"name":"EXCLUDE","optional":true,"followsDelimiter":"/","description":"displays all display tables except those that you specify in the table-list.","type":"standalone"},{"name":"EXCLUDEALL","optional":true,"followsDelimiter":"/","description":"suppresses display of all tables. This option takes precedence over the other options.","type":"standalone"},{"name":"TRACE","optional":true,"followsDelimiter":"/","description":"displays the display table names, labels, and paths.","type":"standalone"}]},{"name":"DISPLAYOUT","description":"enables you to create output tables from your displayed output. This statement is similar to the ODS OUTPUT statement. The table-spec-list specifies a list of output tables to create. Each entry in the list has either a key=value format or a key format: key=value specifies key as the ODS table name, path, or partial pathname, and specifies value as the output table name. key specifies key as the ODS table name and also as the output table name. You cannot specify the ODS table named OutputCasTables in the table-spec-list. You also cannot specify the ODS tables that are created by the AUTOTUNE statement. Table names and partial pathnames are discussed under the DISPLAY statement. The DISPLAYOUT statement does not support regular expressions.","help":"DISPLAYOUT  table-spec-list &lt;/ options&gt;;                                                                   ","arguments":[{"name":"INCLUDEALL","optional":true,"followsDelimiter":"/","description":"creates output tables for all display tables. The name of the created output table is the same as the name of the corresponding display table. If you specify this option, the table-spec-list specification is ignored.","type":"standalone"},{"name":"NOREPLACE","optional":true,"followsDelimiter":"/","description":"does not replace any existing output table of the same name.","type":"standalone"},{"name":"REPEATED","optional":true,"followsDelimiter":"/","description":"replicates all output tables on all nodes.","type":"standalone"}]},{"name":"INPUT","description":"specifies the names of the variables to be used in clustering. You can specify multiple INPUT statements, and you can use the LEVEL=INTERVAL option for interval input variables. For regression, only interval variables are accepted.","help":"INPUT  variables &lt;LEVEL= INTERVAL&gt;;                                              "},{"name":"KERNEL","description":"specifies the kernel-related parameters to be used in the Gaussian process.","help":"KERNEL *kernel-type*&lt;option&gt;;                                              ","arguments":[{"name":"LINEAR","optional":true,"description":"specifies a linear kernel.","type":"standalone","arguments":[{"name":"AUTORELEVANCEDETERMINATION","description":"uses automatic relevance determination in the kernel function.","type":"standalone"},{"name":"FIXKERNELPARMFIRSTITER","description":"sets kernel parameters as fixed values in the first iteration.","type":"standalone"},{"name":"JITTERMAXITERS=","description":"specifies the maximum number of iterations for jitter Cholesky decomposition.","help":"JITTERMAXITERS=*number*","type":"value"}]},{"name":"MATERN32","optional":true,"description":"specifies a Matern 3/2 kernel.","type":"standalone","arguments":[{"name":"AUTORELEVANCEDETERMINATION","description":"uses automatic relevance determination in the kernel function.","type":"standalone"},{"name":"FIXKERNELPARMFIRSTITER","description":"sets kernel parameters as fixed values in the first iteration.","type":"standalone"},{"name":"JITTERMAXITERS=","description":"specifies the maximum number of iterations for jitter Cholesky decomposition.","help":"JITTERMAXITERS=*number*","type":"value"}]},{"name":"MATERN52","optional":true,"description":"specifies a Matern 5/2 kernel.","type":"standalone","arguments":[{"name":"AUTORELEVANCEDETERMINATION","description":"uses automatic relevance determination in the kernel function.","type":"standalone"},{"name":"FIXKERNELPARMFIRSTITER","description":"sets kernel parameters as fixed values in the first iteration.","type":"standalone"},{"name":"JITTERMAXITERS=","description":"specifies the maximum number of iterations for jitter Cholesky decomposition.","help":"JITTERMAXITERS=*number*","type":"value"}]},{"name":"PERIODIC","optional":true,"description":"specifies a periodic kernel.","type":"standalone","arguments":[{"name":"AUTORELEVANCEDETERMINATION","description":"uses automatic relevance determination in the kernel function.","type":"standalone"},{"name":"FIXKERNELPARMFIRSTITER","description":"sets kernel parameters as fixed values in the first iteration.","type":"standalone"},{"name":"JITTERMAXITERS=","description":"specifies the maximum number of iterations for jitter Cholesky decomposition.","help":"JITTERMAXITERS=*number*","type":"value"}]},{"name":"RBF","optional":true,"description":"specifies a radial-basis function kernel.","type":"standalone","arguments":[{"name":"AUTORELEVANCEDETERMINATION","description":"uses automatic relevance determination in the kernel function.","type":"standalone"},{"name":"FIXKERNELPARMFIRSTITER","description":"sets kernel parameters as fixed values in the first iteration.","type":"standalone"},{"name":"JITTERMAXITERS=","description":"specifies the maximum number of iterations for jitter Cholesky decomposition.","help":"JITTERMAXITERS=*number*","type":"value"}]}]},{"name":"OPTIMIZATION","description":"The optimization statement specifies options for the optimization method that you use to train your model.","help":"OPTIMIZATION &lt;*optimization-algorithm*&gt;&lt;options&gt;;                                              ","arguments":[{"name":"ALGORITHM=","optional":true,"aliases":["ALG="],"description":"specifies the optimization-algorithm to use during training. By default, ALGORITHM=ADAM.","help":"ALGORITHM=ADAM &lt;*sgd-options*&gt; | SGD &lt;*sgd-options*&gt;","type":"standaloneOrValue","arguments":[{"name":"ADAM","description":"specifies the adaptive moments (Adam) algorithm, which is one of the variations of the stochastic gradient descent algorithm. It keeps track of the decaying averages of the past gradients and past squared gradients.","help":"ADAM &lt;sgd-options&gt;","type":"standaloneOrValue","arguments":[{"name":"ANNEALINGRATE=","description":"specifies the annealing parameter, β. Annealing is a way to automatically reduce the learning rate as the algorithm progresses, producing smaller steps as the algorithm approaches a solution. By default, ANNEALINGRATE=1.0E–6. The number must be a nonnegative double.","help":"ANNEALINGRATE=*number*","type":"value"},{"name":"COMMFREQ=","description":"specifies the number of minibatches that each computational thread processes before weights are synchronized across all threads and nodes.","help":"COMMFREQ=*number*","type":"value"},{"name":"LEARNINGRATE=","description":"specifies the learning rate parameter, η, for the algorithm. If you see a huge objective value from the algorithm, especially for a small data set, it is likely that the learning rate is set too high. By default, LEARNINGRATE=0.001. The number must be a nonnegative double.","help":"LEARNINGRATE=*number*","type":"value"},{"name":"MINIBATCHSIZE=","description":"specifies the size of the minibatches to use in the algorithm. By default, MINIBATCHSIZE=10.","help":"MINIBATCHSIZE=*number*","type":"value"},{"name":"MOMENTUM=","description":"specifies the value for momentum. The number must be greater than or equal to 0 and less than or equal to 1. By default, MOMENTUM=0.","help":"MOMENTUM=*number*","type":"value"},{"name":"SEED=","description":"specifies the seed for random access of observations on each thread for the algorithm. If number is less than or equal to 0, a random seed is generated by reading the time of day from the computer’s clock.","help":"SEED=*number*","type":"value"},{"name":"USELOCKING","description":"specifies that computational threads share a common weight vector and update weight vector without race conditions. If you omit this option, computational threads update a single weight vector simultaneously. This causes intentional race conditions and nondeterministic behavior but increases performance significantly.","type":"standalone"}]},{"name":"SGD","description":"specifies the plain stochastic gradient descent (SGD) algorithm.","help":"SGD &lt;sgd-options&gt;","type":"standaloneOrValue","arguments":[{"name":"ANNEALINGRATE=","description":"specifies the annealing parameter, β. Annealing is a way to automatically reduce the learning rate as the algorithm progresses, producing smaller steps as the algorithm approaches a solution. By default, ANNEALINGRATE=1.0E–6. The number must be a nonnegative double.","help":"ANNEALINGRATE=*number*","type":"value"},{"name":"COMMFREQ=","description":"specifies the number of minibatches that each computational thread processes before weights are synchronized across all threads and nodes.","help":"COMMFREQ=*number*","type":"value"},{"name":"LEARNINGRATE=","description":"specifies the learning rate parameter, η, for the algorithm. If you see a huge objective value from the algorithm, especially for a small data set, it is likely that the learning rate is set too high. By default, LEARNINGRATE=0.001. The number must be a nonnegative double.","help":"LEARNINGRATE=*number*","type":"value"},{"name":"MINIBATCHSIZE=","description":"specifies the size of the minibatches to use in the algorithm. The number must be greater than or equal to 0 and less than or equal to 1. By default, MOMENTUM=0.","help":"MINIBATCHSIZE=*number*","type":"value"},{"name":"MOMENTUM=","description":"specifies the value for momentum","help":"MOMENTUM=*number*","type":"value"},{"name":"SEED=","description":"specifies the seed for random access of observations on each thread for the algorithm. If number is less than or equal to 0 or not specified, a random seed is generated by reading the time of day from the computer’s clock.","help":"SEED=*number*","type":"value"},{"name":"USELOCKING","description":"specifies that computational threads share a common weight vector and update the weight vector without race conditions. If you omit this option, computational threads update a single weight vector simultaneously. This causes intentional race conditions and nondeterministic behavior but increases performance significantly.","type":"standalone"}]}]},{"name":"MAXITER=","optional":true,"description":"specifies the iteration budget for training. When ALGORITHM=SGD or ADAM, number specifies the desired number of training epochs. By default, MAXITER=250.","help":"MAXITER=*number*","type":"value"},{"name":"MAXTIME=","optional":true,"description":"specifies the maximum time (in seconds) allowed for optimization, where number is greater than or equal to 1. When this value is reached, the optimization terminates the search and returns results. When MAXTIME=0, no maximum time is set. By default, MAXTIME=0.","help":"MAXTIME=*number*","type":"value"},{"name":"REGL1=","optional":true,"description":"specifies the L1 regularization parameter λ1 for the model loss function. The number must be nonnegative. Note that this value is autotuned when you specify the AUTOTUNE statement. By default, REGL1=0.","help":"REGL1=*number*","type":"value"},{"name":"REGL2=","optional":true,"description":"specifies the L2 regularization parameter λ2. The number must be nonnegative. Note that this value is autotuned when you specify the AUTOTUNE statement. By default, REGL2=0.","help":"REGL2=*number*","type":"value"}]},{"name":"OUTPUT","description":"writes the regression score of each observation to the output data table that you specify in the OUT= option.","help":"OUTPUT  OUT=*libref.data-table*&lt;option&gt;;                                              ","arguments":[{"name":"OUT=","optional":true,"description":"names the output data table for PROC GPREG to use. You must specify this option before any other options. libref.data-table is a two-level name, where libref refers to a collection of information that is defined in the LIBNAME statement and includes the library, which includes a path to where the data table is to be stored, and a session identifier, which defaults to the active session but which can be explicitly defined in the LIBNAME statement. data-table specifies the name of the output data table. The output data table contains the scored data. When you specify this option, all variables that are specified in the COPYVARS= option are added to the output data table that is specified in this option.","help":"OUT=*libref.data-table*","type":"dataSet"},{"name":"COPYVAR=","optional":true,"aliases":["COPYVARS="],"description":"lists one or more variables from the input data table that are transferred to the scored output data table, provided that the output data table produces one or more records per input observation. By default, PROC GPREG does not transfer any variables from the input data table to the output data table. The COPYVARS= option accepts numeric and character variables. You can also use the COPYVARS=(_ALL_) option to include all the input variables.","help":"COPYVAR=*variable*","type":"value"}]},{"name":"PARTITION","description":"specifies how observations in the input data set are logically partitioned into disjoint subsets for model training, validation, and testing. Either you can designate a variable in the input data table and a set of formatted values of that variable to determine the role of each observation, or you can specify proportions to use for randomly assigning observations to each role.","help":"PARTITION  partition-option;                                              ","arguments":[{"name":"FRACTION","optional":true,"description":"randomly assigns specified proportions of the observations in the input data table to the roles. You specify the proportions for testing and validation by using the TEST= and VALIDATE= suboptions. If you specify both the TEST= and VALIDATE= suboptions, then the sum of the specified fractions must be less than 1 and the remaining fraction is the proportion of the observations assigned to the training role. The SEED= option specifies an integer that is used to start the pseudorandom number generator for random partitioning of data for training, testing, and validation. If you do not specify SEED=number or if number is less than or equal to 0, the seed is generated by reading the time of day from the computer’s clock. Note: You cannot specify the FRACTION option and the AUTOTUNE statement at the same time.","help":"FRACTION(&lt;TEST=*fraction*&gt; &lt;VALIDATE=*fraction*&gt; &lt;SEED=*number*&gt;)","type":"standaloneOrValue"},{"name":"ROLE=","optional":true,"aliases":["ROLEVAR="],"description":"names the variable in the input data table whose values are used to assign roles to each observation. This variable cannot also appear as an analysis variable in other statements or options. The TEST=, TRAIN=, and VALIDATE= suboptions specify the formatted values of this variable that are used to assign observation roles. If you do not specify the TRAIN= suboption, then all observations whose role is not determined by the TEST= or VALIDATE= suboption are assigned to the training role.","help":"ROLE=*variable* (&lt;TEST=*'value'*&gt; &lt;TRAIN=*'value'*&gt; &lt;VALIDATE=*'value'*&gt;) ","type":"value"}]},{"name":"SAVESTATE","description":"creates an analytic store for the model and saves it as a binary object in a data table. You can then use the analytic store in the ASTORE procedure to score new data. The analytic store inherits the same parameters that PROC GPREG uses.","help":"SAVESTATE  RSTORE=*libref.data-table*;                                              ","arguments":[{"name":"RSTORE=","optional":true,"description":"specifies a data table in which to save the analytic store for the model. libref.data-table is a two-level name, where libref refers to the library, and data-table specifies the name of the output data table.","help":"RSTORE=*libref.data-table*","type":"dataSet"}]},{"name":"TARGET","description":"specifies the name of the variable to be used as the regression target. You can use one TARGET statement that includes the LEVEL=INTERVAL option for the interval target variable. Only one interval variable is accepted as the target.","help":"TARGET  variable &lt;LEVEL= INTERVAL&gt;;                                              "}],"supportSiteInformation":{"docsetId":"casml","docsetVersion":"latest","docsetTargetFile":"casml_gpreg_toc.htm"}}