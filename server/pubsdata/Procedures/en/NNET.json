{"name":"NNET","statements":[{"name":"PROC NNET","description":"The NNET procedure trains a multilayer perceptron neural network in SAS Viya. For more information about multilayer perceptron neural networks, see Bishop (1995). PROC NNET can also use a previously trained network to score a data table (referred to as stand-alone scoring), or it can generate SAS DATA step statements that can be used to score a data table.","help":"PROC NNET <DATA=CAS-libref.data-table> <INMODEL=< CAS-libref. >data-table> <MISSING=MIN | MAX | MEAN> <NTHREADS=number-of-threads> <STANDARDIZE=NONE | STD | MIDRANGE>;     \n\tARCHITECTURE <GLM> <MLP> <MLP DIRECT> ...;     \n\tAUTOTUNE <FRACTION=number> <KFOLD=number> <MAXEVALS=number> ...; \n   \n\tCODE <FILE=filename> <NOCOMPPGM> ; \n   \n\tCROSSVALIDATION <KFOLD=number> ; \n   \n\tHIDDEN <ACT=<EXP | IDENTITY | LOGISTIC>... > <COMB=<ADD | LINEAR>> ; \n   \n\tINPUT <LEVEL=<INT | NOM>> ; \n   \n\tOPTIMIZATION <ALGORITHM=<LBFGS | SGD>> <MAXITER=number> <MAXTIME=number> ...; \n   \n\tPARTITION <FRACTION(VALIDATE=fraction TEST=fraction < SEED=random-seed >)> <ROLEVAR=variable(TRAIN=value VALIDATE=value < TEST=value >)> ;  \n  \n\tSCORE <COPYVAR=variable | COPYVARS=(variables)> <OUT=CAS-libref.data-table> ; \n   \n\tTARGET <ACT=<EXP | IDENTITY | SIN>... > <ERROR=<ENTROPY | GAMMA | NORMAL>... > <LEVEL=<INT | NOM>> ...; \n   \n\tTRAIN <NUMTRIES=number> <OUTMODEL=CAS-libref.data-table> <VALIDATION=CAS-libref.data-table> ...; \n   \n\tWEIGHT variable; ","arguments":[{"name":"DATA=","optional":true,"description":"Names the input data table for PROC NNET to use. The default is the most recently created data table. CAS-libref.data-table is a two-level name, where","help":"DATA=*CAS-libref.data-table*","type":"dataSet"},{"name":"INMODEL=","optional":true,"description":"Specifies a model for stand-alone scoring or coding.","type":"value"},{"name":"MISSING=","optional":true,"description":"Specifies the statistic with which to impute missing values for interval inputs. If you specify this option, missing values for nominal inputs and targets are both treated as a valid level. By default, observations that have missing values for input variables are excluded from the analysis.","help":"MISSING=MIN | MAX | MEAN","type":"choice","arguments":[{"name":"MIN","description":"Specifies the MIN statistic.","type":"standalone"},{"name":"MAX","description":"Specifies the MAX statistic.","type":"standalone"},{"name":"MEAN","description":"Specifies the MEAN statistic.","type":"standalone"}]},{"name":"NTHREADS=","optional":true,"description":"Specifies the number of threads to use for the computation. The default value is the number of CPUs available. The value of number-of-threads can be from 1 to 64, inclusive.","help":"NTHREADS=*number-of-threads*","type":"value"},{"name":"STANDARDIZE=","optional":true,"description":"Specifies the method to use for standardizing interval inputs.","help":"STANDARDIZE=NONE | STD | MIDRANGE","type":"choice","arguments":[{"name":"NONE","description":"Specifies the method in which the variables are not altered.","type":"standalone"},{"name":"STD","description":"Specifies the method in which the variables are scaled such that their mean is 0 and the standard deviation is 1.","type":"standalone"},{"name":"MIDRANGE","description":"Specifies the method in which the variables are scaled such that their midrange is 0 and the half-range is 1. That is, the variables have a minimum of –1 and a maximum of 1.","type":"standalone"}]}]},{"name":"ARCHITECTURE","description":"The ARCHITECTURE statement specifies the architecture of the neural network to be trained.","help":"ARCHITECTURE &lt;GLM&gt; &lt;MLP&gt; &lt;MLP DIRECT&gt; ...","arguments":[{"name":"GLM","optional":true,"description":"Specifies a neural network architecture that has no hidden layers (this is equivalent to a generalized linear model). If you specify this architecture-option, the HIDDEN statement is not allowed.","type":"standalone"},{"name":"MLP","optional":true,"description":"Specifies a multilayer perceptron architecture that has one or more hidden layers. This is the default architecture.","type":"standalone"},{"name":"MLP DIRECT","optional":true,"description":"Specifies that direct connections between each input and each target neuron be included when the MLP architecture is used.","type":"standalone"}]},{"name":"AUTOTUNE","description":"The AUTOTUNE statement activates the tuning optimization algorithm, which searches for the best hidden layers and regularization parameters based on the problem and specified options. If ALGORITHM=SGD, the algorithm also searches for the best values of the learning rate and annealing rate. When you specify AUTOTUNE statement, PROC NNET ignores any HIDDEN statements that are specified. You cannot specify the AUTOTUNE statement with the CROSSVALIDATION statement, the PARTITION statement, or the VALIDATION= option in the TRAIN statement.","help":"AUTOTUNE &lt;FRACTION=number&gt; &lt;KFOLD=number&gt; &lt;MAXEVALS=number&gt; ...","arguments":[{"name":"FRACTION=","optional":true,"description":"Specifies the fraction of all data to be used for validation, where number must be between 0.01 and 0.99, inclusive. If you specify this option, the tuner uses a single partition validation for finding the objective value (validation error estimate). This option might not be advisable for small or unbalanced data tables where the random assignment of the validation subset might not provide a good estimate of error. For large, balanced data tables, a single validation partition is usually sufficient for estimating error; a single partition is more efficient than cross validation in terms of the total execution time. By default, FRACTION=0.3. You cannot specify this option in combination with the KFOLD= option.","help":"FRACTION=*number*","type":"value"},{"name":"KFOLD=","optional":true,"description":"Specifies the number of partition folds in the cross validation process, where number must be between 2 and 20, inclusive. If you specify this option, the tuner uses cross validation to find the objective value. In cross validation, each model evaluation requires number of training executions (on number–1 data folds) and number of scoring executions (on 1 hold-out fold). Thus, the evaluation time is increased by approximately number. For small to medium data tables or for unbalanced data tables, cross validation provides on average a better representation of error across the entire data table (a better generalization error). By default, KFOLD=5. You cannot specify this option in combination with the FRACTION= option.","help":"KFOLD=*number*","type":"value"},{"name":"MAXEVALS=","optional":true,"description":"Specifies the maximum number of configuration evaluations allowed for the tuner, where number must be an integer greater than or equal to 3. When the number of evaluations is reached, the tuner terminates the search and returns the results. To produce a single objective function value (validation error estimate), each configuration evaluation requires either a single model training and scoring execution on a validation partition, or a number of training and scoring executions equal to the value of the KFOLD= option for cross validation. The MAXEVALS= option might lead to termination before the value of the MAXITER= option or the MAXTIME= option is reached. By default, MAXEVALS=50.","help":"MAXEVALS=*number*","type":"value"},{"name":"MAXITER=","optional":true,"description":"Specifies the maximum number of iterations of the optimization tuner, where number must be greater than or equal to 1. Each iteration normally involves a number of objective evaluations up to the value of the POPSIZE= option. The MAXITER= option might lead to termination before the value of the MAXEVALS= option or the MAXTIME= option is reached.","help":"MAXITER=*number*","type":"value"},{"name":"MAXTIME=","optional":true,"description":"Specifies the maximum time (in seconds) allowed for the tuner, where number must be greater than or equal to 1. When this value is reached, the tuner terminates the search and returns results. The actual run time for optimization might be longer because it includes the remaining time needed to finish the current evaluation. For long-running model training (large data tables), the actual run time might significantly exceed number. The MAXTIME= option might lead to termination before the value of the MAXEVALS= option or the MAXITER= option is reached. By default, MAXTIME=36000.","help":"MAXTIME=*number*","type":"value"},{"name":"POPSIZE=","optional":true,"description":"Specifies the maximum number of evaluations in one iteration (population), where number must be greater than or equal to 1. In some cases, the tuner algorithm might generate a number of new configurations smaller than number. By default, POPSIZE=10.","help":"POPSIZE=*number*","type":"value"}]},{"name":"CODE","description":"The CODE statement returns the SAS score code that can be used to score data similar to the input data.","help":"CODE &lt;FILE=filename&gt; &lt;NOCOMPPGM&gt; ","arguments":[{"name":"FILE=","optional":true,"description":"Specifies the name of the file where PROC NNET is to write the SAS score code.","help":"FILE=*filename*","type":"value"},{"name":"NOCOMPPGM","optional":true,"description":"Omits the logic of the option FRACTION= option in the PARTITION statement from the score code. If you do not specify this option, the logic of the option FRACTION= option in the PARTITION statement is included in the score code.","type":"standalone"}]},{"name":"CROSSVALIDATION","description":"The CROSSVALIDATION statement performs a k-fold cross validation process to find the average estimated validation error. You cannot specify the CROSSVALIDATION statement if you specify either the AUTOTUNE statement or the PARTITION statement.","help":"CROSSVALIDATION &lt;KFOLD=number&gt; ","arguments":[{"name":"KFOLD=","optional":true,"description":"Specifies the number of partition folds in the cross validation process, where number must be between 2 and 20, inclusive. By default, KFOLD=5.","help":"KFOLD=*number*","type":"value"}]},{"name":"HIDDEN","description":"The HIDDEN statement specifies the number of neurons or units in a hidden layer. You can specify multiple HIDDEN statements; each HIDDEN statement represents a hidden layer. HIDDEN statements are ignored when you specify the AUTOTUNE statement.","help":"HIDDEN &lt;ACT=&lt;EXP | IDENTITY | LOGISTIC&gt;... &gt; &lt;COMB=&lt;ADD | LINEAR&gt;&gt; ","arguments":[{"name":"ACT=","optional":true,"followsDelimiter":"/","description":"Specifies the activation function for the hidden layer.","help":"ACT=EXP | IDENTITY | LOGISTIC | SIN | TANH","type":"choice","arguments":[{"name":"EXP","followsDelimiter":"/","description":"Specifies the exponential function.","type":"standalone"},{"name":"IDENTITY","followsDelimiter":"/","description":"Specifies the identity function.","type":"standalone"},{"name":"LOGISTIC","followsDelimiter":"/","description":"Specifies the logistic function.","type":"standalone"},{"name":"SIN","followsDelimiter":"/","description":"Specifies the sine function.","type":"standalone"},{"name":"TANH","followsDelimiter":"/","description":"Specifies the hyperbolic tangent function.","type":"standalone"}]},{"name":"COMB=","optional":true,"followsDelimiter":"/","description":"Specifies the combination function for the hidden layer.","help":"COMB=ADD | LINEAR","type":"choice","arguments":[{"name":"ADD","followsDelimiter":"/","description":"Specifies the additive combination function.","type":"standalone"},{"name":"LINEAR","followsDelimiter":"/","description":"Specifies the linear combination function.","type":"standalone"}]}]},{"name":"INPUT","description":"The INPUT statement identifies the variables in the input data table that are input to the neural network. You can specify multiple INPUT statements.","help":"INPUT &lt;LEVEL=&lt;INT | NOM&gt;&gt; ","arguments":[{"name":"LEVEL=","optional":true,"followsDelimiter":"/","description":"Specifies the variables in the input data table.","help":"LEVEL=INT | NOM","type":"choice","arguments":[{"name":"INT","followsDelimiter":"/","description":"Specifies that the variables are interval variables, which must be numeric.","type":"standalone"},{"name":"NOM","followsDelimiter":"/","description":"Specifies that the variables are nominal variables, also known as classification variables, which can be numeric or character.","type":"standalone"}]}]},{"name":"OPTIMIZATION","description":"The OPTIMIZATION statement specifies options for the optimization method that is used to train your model.","help":"OPTIMIZATION &lt;ALGORITHM=&lt;LBFGS | SGD&gt;&gt; &lt;MAXITER=number&gt; &lt;MAXTIME=number&gt; ...","arguments":[{"name":"ALGORITHM=","optional":true,"description":"Specifies the optimization algorithm to use during training.","help":"ALGORITHM=LBFGS | SGD","type":"choice","arguments":[{"name":"LBFGS","description":"Specifies the limited-memory Broyden-Fletcher-Goldfarb-Shanno algorithm.","type":"standalone"},{"name":"SGD","description":"Specifies the stochastic gradient descent algorithm. When ALGORITHM=SGD, you can specify these additional sgd-options: ANNEALINGRATE=number specifies the annealing parameter, β. Annealing is a way to automatically reduce the learning rate as SGD progresses, causing smaller steps as SGD approaches a solution. COMMFREQ=number specifies the number of minibatches that each computational thread processes before weights are synchronized across all threads and nodes. LEARNINGRATE=number specifies the learning rate parameter, η, for SGD. MINIBATCHSIZE=number specifies the size of the minibatches used in SGD. MOMENTUM=number specifies the value for momentum. The number must be greater than or equal to 0 and less than or equal to 1. By default, MOMENTUM=0. SEED=number specifies the seed for random access of observations on each thread for the SGD algorithm. USELOCKING specifies that computational threads share a common weight vector and update weight vector without race conditions.","type":"standalone"}]},{"name":"MAXITER=","optional":true,"description":"Specifies the maximum number of iterations or epochs before the algorithm terminates.","help":"MAXITER=*number*","type":"value"},{"name":"MAXTIME=","optional":true,"description":"Specifies the iteration budget for training. For LBFGS, the algorithm stops after MAXITER= iterations if convergence has not been achieved. For SGD, number specifies the desired number of training epochs. When MAXTIME=0, no maximum time is set.","help":"MAXTIME=*number*","type":"value"},{"name":"REGL1=","optional":true,"description":"Specifies the L1 regularization parameter 1 for the model loss function. The number must be nonnegative. Note that this value is autotuned when you specify the AUTOTUNE statement.","help":"REGL1=*number*","type":"value"},{"name":"REGL2=","optional":true,"description":"Specifies the L2 regularization parameter 2. The number must be nonnegative. Note that this value is autotuned when you specify the AUTOTUNE statement.","help":"REGL2=*number*","type":"value"}]},{"name":"PARTITION","description":"The PARTITION statement specifies how observations in the input data set are logically partitioned into disjoint subsets for model training, validation, and testing. Either you can designate a variable in the input data table and a set of formatted values of that variable to determine the role of each observation, or you can specify proportions to use for random assignment of observations for each role. Alternatively, you can use a separate validation data table in the TRAIN statement to do validation.","help":"PARTITION &lt;FRACTION(VALIDATE=fraction TEST=fraction &lt; SEED=random-seed &gt;)&gt; &lt;ROLEVAR=variable(TRAIN=value VALIDATE=value &lt; TEST=value &gt;)&gt; ","arguments":[{"name":"FRACTION=","optional":true,"description":"Randomly assigns the specified proportions of the observations in the input data table to training and validation roles. You specify the proportions for testing and validation by using the TEST= and VALIDATE= suboptions. The VALIDATE= suboption is required, and the TEST= suboption is optional. If you specify both the TEST= and VALIDATE= suboptions, then the sum of the specified fractions must be less than 1 and the remaining fraction of the observations are assigned to the training role. Otherwise, the PARTITION statement is ignored. The range of the VALIDATE= and TEST= suboptions is from 1E–5 to 1 – (1E–5), inclusive.","help":"FRACTION=VALIDATE= | TEST= | SEED=","type":"choice","arguments":[{"name":"VALIDATE=","type":"value"},{"name":"TEST=","type":"value"},{"name":"SEED=","type":"value"}]},{"name":"ROLEVAR=","optional":true,"description":"Names the variable in the input data table whose values are used to assign roles to each observation. The formatted values of this variable, which are used to assign observations roles, are specified in the TEST=, TRAIN=, and VALIDATION= suboptions. The TRAIN= and VALIDATE= suboptions are required; the TEST=suboption is optional.","help":"ROLEVAR=TRAIN= | VALIDATE= | TEST=","type":"choice","arguments":[{"name":"TRAIN=","type":"value"},{"name":"VALIDATE=","type":"value"},{"name":"TEST=","type":"value"}]}]},{"name":"SCORE","description":"The SCORE statement creates a new data table that is the result of prediction from using the input data and the model.","help":"SCORE &lt;COPYVAR=variable | COPYVARS=(variables)&gt; &lt;OUT=CAS-libref.data-table&gt; ","arguments":[{"name":"COPYVAR=","optional":true,"aliases":["COPYVARS="],"description":"Lists one or more variables from the input data table to be transferred to the output data table.","type":"value"},{"name":"OUT=","optional":true,"description":"Names the output data table for PROC NNET to use. CAS-libref.data-table is a two-level name, where","help":"OUT=*CAS-libref.data-table*","type":"dataSet"}]},{"name":"TARGET","description":"The TARGET statement specifies the target variable for the neural network. This statement is required.","help":"TARGET &lt;ACT=&lt;EXP | IDENTITY | SIN&gt;... &gt; &lt;ERROR=&lt;ENTROPY | GAMMA | NORMAL&gt;... &gt; &lt;LEVEL=&lt;INT | NOM&gt;&gt; ...","arguments":[{"name":"ACT=","optional":true,"followsDelimiter":"/","description":"Specifies the activation function for the target.","help":"ACT=EXP | IDENTITY | SIN | SOFTMAX | TANH","type":"choice","arguments":[{"name":"EXP","followsDelimiter":"/","description":"Specifies the exponential function. You can use ACT=EXP only with ERROR= GAMMA or ERROR=POISSON.","type":"standalone"},{"name":"IDENTITY","followsDelimiter":"/","description":"Specifies the identity function..","type":"standalone"},{"name":"SIN","followsDelimiter":"/","description":"Specifies the sine function.","type":"standalone"},{"name":"SOFTMAX","followsDelimiter":"/","description":"Specifies the softmax function.","type":"standalone"},{"name":"TANH","followsDelimiter":"/","description":"Specifies the hyperbolic tangent function.","type":"standalone"}]},{"name":"COMB=","optional":true,"followsDelimiter":"/","description":"Specifies the combination function for the target layer.","help":"COMB=ADD | LINEAR","type":"choice","arguments":[{"name":"ADD","followsDelimiter":"/","description":"Specifies the additive combination function.","type":"standalone"},{"name":"LINEAR","followsDelimiter":"/","description":"Specifies the linear combination function.","type":"standalone"}]},{"name":"ERROR=","optional":true,"followsDelimiter":"/","description":"Specifies the error function. The entropy error function is used only when LEVEL=NOM.","help":"ERROR=ENTROPY | GAMMA | NORMAL | POISSON","type":"choice","arguments":[{"name":"ENTROPY","followsDelimiter":"/","description":"Specifies tthe cross-entropy function.","type":"standalone"},{"name":"GAMMA","followsDelimiter":"/","description":"Specifies the gamma error function. This function is usually used when you want to predict the time between events. Only ACT=EXP is valid when ERROR=GAMMA.","type":"standalone"},{"name":"NORMAL","followsDelimiter":"/","description":"Specifies the normal error function, which is the sum of the squared differences between the network output and the target value.","type":"standalone"},{"name":"POISSON","followsDelimiter":"/","description":"Specifies the Poisson error function. This function is usually used when you want to predict the number of events per unit time. Only ACT=EXP is valid when ERROR=POISSON.","type":"standalone"}]},{"name":"LEVEL=","optional":true,"followsDelimiter":"/","description":"Specifies the variable type.","help":"LEVEL=INT | NOM","type":"choice","arguments":[{"name":"INT","followsDelimiter":"/","description":"Specifies that the variable is interval, which must be numeric","type":"standalone"},{"name":"NOM","followsDelimiter":"/","description":"Specifies that the variable is nominal, also known as a classification variable, which can be numeric or character.","type":"standalone"}]}]},{"name":"TRAIN","description":"The TRAIN statement causes the NNET procedure to use the training data that are specified in the PROC NNET statement to train a neural network model whose structure is specified in the ARCHITECTURE, INPUT, TARGET, and HIDDEN statements. The goal of training is to determine a set of network weights that best predicts the targets in the training data while still doing a good job of predicting targets of unseen data (that is, generalizing well and not overfitting).","help":"TRAIN &lt;NUMTRIES=number&gt; &lt;OUTMODEL=CAS-libref.data-table&gt; &lt;VALIDATION=CAS-libref.data-table&gt; ...","arguments":[{"name":"NUMTRIES=","optional":true,"description":"Syntax: NUMTRIES=number specifies the number of times the network is to be trained using a different starting point. Specifying this option helps ensure that the optimizer finds the table of weights that truly minimizes the objective function and does not return a local minimum. The value of number must be an integer between 1 and 20,000, inclusive. By default, NUMTRIES=1.","help":"NUMTRIES=*number*","type":"value"},{"name":"OUTMODEL=","optional":true,"description":"Specifies the final model from training. These parameters include the network architecture, input and target variable names and types, and trained weights. You can use the model data table later to score a different input data table as long as the variable names and types of the variables in the new input data table match those in the training data table.","help":"OUTMODEL=*CAS-libref.data-table*","type":"dataSet"},{"name":"VALIDATION=","optional":true,"description":"Specifies a separate data table for validation during training. If you specify both the VALIDATION= and PARTITION statements, the PARTITION statement is ignored. If neither of these statements is specified, training is done without the validation data table. The VALIDATION= data table must have the same variables that you specify in the DATA= option. NOTE: The DATA options keep drop tempnames and tempexpress are not supported.","help":"VALIDATION=*CAS-libref.data-table*","type":"dataSet"},{"name":"WSEED=","optional":true,"aliases":["SEED="],"description":"Specifies the seed for generating initial random weights. If you do not specify a seed or you specify a value less than or equal to 0, the seed is generated from the computer clock. This option enables you to reproduce the same sample output.","type":"value"}]},{"name":"WEIGHT","description":"If you specify a WEIGHT statement, variable identifies a numeric variable in the input data table that contains the weight to be placed on the prediction error (the difference between the output of the network and the target value specified in the input data table) for each observation during training.","help":"WEIGHT variable"}],"supportSiteInformation":{"docsetId":"casml","docsetVersion":"v_033","docsetTargetFile":"casml_nnet_toc.htm"}}